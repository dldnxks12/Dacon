{"nbformat":4,"nbformat_minor":0,"metadata":{"hide_input":false,"kernelspec":{"name":"python3","display_name":"Python 3.8.10 64-bit ('ds': virtualenv)"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"colab":{"name":"cnn1d_lstm_stacking.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"accelerator":"GPU","interpreter":{"hash":"29b29da72ccf9add6db5fb093767ef4479768a661c02b23b62be221c8041a8dc"}},"cells":[{"cell_type":"markdown","metadata":{"id":"sm0xZ2uXk6Xg","executionInfo":{"status":"ok","timestamp":1633575234019,"user_tz":-540,"elapsed":16808,"user":{"displayName":"꾸꾸","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14922489440020274566"}},"outputId":"eac1fb83-be3f-4529-e743-2d5df5ac1009"},"source":["## Reference\n","\n","* 데이터 증강:\n","  * terryum 님의 웨어러블 센서 데이터 증강 기법 - https://github.com/terryum/Data-Augmentation-For-Wearable-Sensor-Data \n","\n","* 모델구현\n","  * https://machinelearningmastery.com/how-to-develop-rnn-models-for-human-activity-recognition-time-series-classification/?utm_source=pocket_mylist\n","    1. LSTM 단독모델\n","    2. CNN-LSTM 결합모델\n","    3. ConvLSTM 모델\n","    비교 3가지 돌려봤는데 CNN-LSTM 이 가장 성능이 좋아서 선택\n","  * CNN단독 모델 보다 CNN-LSTM 모델이 우수한 이유 설명 - 미세먼지 예측 성능 개선을 위한 CNN-LSTM 결합 방법 http://koreascience.or.kr/article/JAKO202005653789386.page\n","  * 그외 시계열 데이터 LSTM 응용 방법 - https://machinelearningmastery.com/how-to-develop-lstm-models-for-time-series-forecasting/ \n","\n","* 교차검증 및 성능 강화\n","  * early stopping 과 callback 을 사용하여 과적합을 방지할 수 있다. - https://tykimos.github.io/2017/07/09/Early_Stopping/ \n","  * 앙상블 기법\n","    * softvoting\n","    * stacking - https://machinelearningmastery.com/stacking-ensemble-for-deep-learning-neural-networks/"]},{"cell_type":"code","metadata":{"id":"M-uLvsp3A4kn"},"source":["# 구글 드라이브 마운트\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jmIJ4F3Lk6Il"},"source":["# 기본 directory 설정\n","import os\n","os.chdir('/content/drive/MyDrive/Monthly_Workout')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c6d7M76Wl5Bx"},"source":["# 모듈 불러오기\n","import random\n","import pandas as pd\n","import numpy as np\n","from tqdm import tqdm\n","from math import pi\n","import matplotlib.pyplot as plt\n","from scipy.interpolate import CubicSpline"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gFXySamQk5y-"},"source":["# 데이터 불러오기\n","path = './' # 기본 directory 경로에 추가 할 경로\n","\n","train = pd.read_csv(path + 'train_features.csv')\n","train_labels = pd.read_csv(path + 'train_labels.csv')\n","test = pd.read_csv(path + 'test_features.csv')\n","submission = pd.read_csv(path + 'sample_submission.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BhYkJBjwH6XW"},"source":["train"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"m7Ncehkzk22p"},"source":["act_list=train.iloc[:,2:].columns\n","acc_list=['acc_x','acc_y','acc_z']\n","gy_list=['gy_x','gy_y','gy_z']\n","act_list"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sK8gCKp3lp3D"},"source":["# acc 데이터와 gy 데이터로 분할\n","def sensor_split(data):\n","    X_acc = []\n","    X_gy = []\n","\n","    for i in tqdm(data['id'].unique()):\n","        temp_acc = np.array(data[data['id'] == i].loc[:,acc_list])\n","        temp_gy = np.array(data[data['id'] == i].loc[:,gy_list])\n","        X_acc.append(temp_acc)\n","        X_gy.append(temp_gy)\n","      \n","    X_acc = np.array(X_acc).reshape(-1,600,3)\n","    X_gy = np.array(X_gy).reshape(-1,600,3)\n","\n","    return X_acc, X_gy"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6p-mjEZaltsK"},"source":["# 데이터 증강\n","def aug(data, uid, shift):\n","    shift_data = np.roll(data[uid], shift, axis=0)\n","    return shift_data\n","def rolling(data):\n","  aug_data=[]\n","  for i in range(data.shape[0]):\n","    temp=list((aug(data,i,int(random.random()*600))))\n","    aug_data.append(temp)\n","  return np.array(aug_data)\n","\n","sigma = 0.2\n","knot = 4\n","def GenerateRandomCurves(X, sigma, knot=4):\n","    xx = (np.ones((X.shape[1],1))*(np.arange(0,X.shape[0], (X.shape[0]-1)/(knot+1)))).transpose()\n","    yy = np.random.normal(loc=1.0, scale=sigma, size=(knot+2, X.shape[1]))\n","    x_range = np.arange(X.shape[0])\n","    cs_x = CubicSpline(xx[:,0], yy[:,0])\n","    cs_y = CubicSpline(xx[:,1], yy[:,1])\n","    cs_z = CubicSpline(xx[:,2], yy[:,2])\n","    return np.array([cs_x(x_range),cs_y(x_range),cs_z(x_range)]).transpose()\n","\n","# Time Warping\n","sigma = 0.2\n","knot = 4\n","def DistortTimesteps(X, sigma):\n","    tt = GenerateRandomCurves(X, sigma) # Regard these samples aroun 1 as time intervals\n","    tt_cum = np.cumsum(tt, axis=0)        # Add intervals to make a cumulative graph\n","    # Make the last value to have X.shape[0]\n","    t_scale = [(X.shape[0]-1)/tt_cum[-1,0],(X.shape[0]-1)/tt_cum[-1,1],(X.shape[0]-1)/tt_cum[-1,2]]\n","    tt_cum[:,0] = tt_cum[:,0]*t_scale[0]\n","    tt_cum[:,1] = tt_cum[:,1]*t_scale[1]\n","    tt_cum[:,2] = tt_cum[:,2]*t_scale[2]\n","    return tt_cum\n","\n","def TimeWarp(X, sigma):\n","    tt_new = DistortTimesteps(X, sigma)\n","    X_new = np.zeros(X.shape)\n","    x_range = np.arange(X.shape[0])\n","    X_new[:,0] = np.interp(x_range, tt_new[:,0], X[:,0])\n","    X_new[:,1] = np.interp(x_range, tt_new[:,1], X[:,1])\n","    X_new[:,2] = np.interp(x_range, tt_new[:,2], X[:,2])\n","    return X_new\n","\n","def ts(data, method,sigma):\n","    new_data=[]\n","    for i in range(data.shape[0]):\n","        temp=list(method(data[i], sigma))\n","        new_data.append(temp)\n","    return np.array(new_data)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HECv9WwUTQgZ"},"source":["# 데이터 증강 (반복하고 싶은 만큼 조정)\n","def start_augmentation(train, train_labels):\n","    # acc, gy 데이터 분할\n","    X_train_mod=pd.merge(train,train_labels,how='left',on='id')\n","    X_train_acc, X_train_gy= sensor_split(X_train_mod)\n","\n","    # 증강시키고 추가할 임시 데이터 복사본\n","    X_train_acc_temp = X_train_acc.copy()\n","    X_train_gy_temp = X_train_gy.copy()\n","\n","    # label 데이터 변환\n","    y_train = train_labels['label']\n","    y_train_total = np.append(y_train, y_train, axis=0)\n","\n","    rep = 3 # 5이상의 경우 reshape 과정에서 reset될 가능성 높음\n","    for i in range(rep):\n","        X_train_acc_roll = rolling(X_train_acc_temp) # Rolling 만\n","        X_train_acc_rt = ts(rolling(X_train_acc_temp), TimeWarp, 0.2) # 롤링한 데이터에 Time Warping\n","\n","        X_train_gy_roll = rolling(X_train_gy_temp)\n","        X_train_gy_rt = ts(rolling(X_train_gy_temp), TimeWarp, 0.2)\n","\n","        # 증강시킨 데이터 원래 데이터에 추가\n","        X_train_acc = np.append(X_train_acc, X_train_acc_roll, axis=0)\n","        X_train_acc = np.append(X_train_acc, X_train_acc_rt, axis=0)\n","        X_train_gy = np.append(X_train_gy, X_train_gy_roll, axis=0)\n","        X_train_gy = np.append(X_train_gy, X_train_gy_rt, axis=0)\n","\n","        y_train_total = np.append(y_train_total, y_train, axis=0) # label 데이터 복제\n","        if i != (rep-1): # 마지막 한 번 제외\n","            y_train_total = np.append(y_train_total, y_train, axis=0)\n","\n","    return X_train_acc, X_train_gy, y_train_total "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pwbF7XSAyy4l"},"source":["X_train_acc, X_train_gy, y_train_total = start_augmentation(train, train_labels)\n","\n","print(X_train_acc.shape, X_train_gy.shape, y_train_total.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qr_nngW_rQ4F"},"source":["# train 데이터만 Grdient (배열 형태로 있기 때문에 진행하고 나중에 합치기)\n","grad_acc = np.gradient(X_train_acc, axis=0)\n","grad_gy = np.gradient(X_train_gy, axis=0)\n","\n","grad_acc.shape, grad_gy.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YypFWX080Va3"},"source":["# test 데이터만 Gradient\n","feature_names = ['acc_x','acc_y','acc_z','gy_x','gy_y','gy_z']\n","\n","grad_cols=[]\n","for col in feature_names:\n","    grad_cols.append(f\"grad_{col}\")\n","\n","total_feature_names = feature_names + grad_cols\n","\n","for uid in tqdm(test['id'].unique()):\n","    temp = test.loc[test['id']==uid, feature_names]\n","    grad = np.gradient(temp, axis=0)\n","    test.loc[test['id']==uid, grad_cols] = grad\n","    \n","test"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"f_jVLRgKnGeU"},"source":["# np array 형태를 dataframe 으로 변환\n","def np_to_df(X_train_acc, X_train_gy):\n","    acc = [e for sl in X_train_acc for e in sl]\n","    gy = [e for sl in X_train_gy for e in sl]\n","    acc_grad = [e for sl in grad_acc for e in sl]\n","    gy_grad = [e for sl in grad_gy for e in sl]\n","\n","    df_report_acc = np.stack(acc, axis = 0)\n","    df_report_gy = np.stack(gy, axis = 0)\n","    df_report_acc_grad = np.stack(acc_grad, axis = 0)\n","    df_report_gy_grad = np.stack(gy_grad, axis = 0)\n","\n","    df_acc = pd.DataFrame(df_report_acc, columns= ['acc_x', 'acc_y', 'acc_z']) \n","    df_gy = pd.DataFrame(df_report_gy, columns= ['gy_x', 'gy_y', 'gy_z']) \n","    df_acc_grad = pd.DataFrame(df_report_acc_grad, columns= ['grad_acc_x', 'grad_acc_y', 'grad_acc_z']) \n","    df_gy_grad = pd.DataFrame(df_report_gy_grad, columns= ['grad_gy_x', 'grad_gy_y', 'grad_gy_z']) \n","\n","    # acc, gy 데이터프레임 병합\n","    df_aug_result = pd.concat([df_acc, df_gy, df_acc_grad, df_gy_grad], axis = 1)\n","    \n","    return df_aug_result"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"T0qgbFfRnO_g"},"source":["train = np_to_df(X_train_acc, X_train_gy)\n","train"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qQnIl4SbEYxv"},"source":["# 시각화를 위해서 id 로 묶어주기 위해 잠시 생성\n","val_id = []\n","n = int(len(train))\n","\n","# 600번씩 반복되도록 임의로 배열 생성\n","for i in range(n//600):\n","    for j in range(600):\n","        val_id.append(i)\n","\n","train.insert(0, 'id', val_id) # 리스트값 id 열에 붙여넣기\n","train.head(603)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y4bZIlInMpAs"},"source":["### 증강시킨 데이터 비교해보기\n","1. 원래 id 0 번째의 데이터 - Shoulder Press (dumbell)\n","2. id 0 의 데이터를 Rolling 한 데이터\n","3. id 0 의 데이터를 Rolling -> Time Warping 한 데이터  \n","\n","4, 5, 6 반복\n","\n","확인해보면 파형은 비슷하지만 약간씩 변형된 데이터를 확인할 수 있다.  \n","그러므로 같은 동작이라고 판단하고 label 데이터를 붙여주어 학습시켰다."]},{"cell_type":"code","metadata":{"id":"4GmxNsdoNDGi"},"source":["n_id = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10] # 원하는 id 별 증강 데이터 확인 (0의 경우 0 번째 데이터)\n","fig,ax=plt.subplots(nrows=2,ncols=3, figsize=(16, 10))\n","ex1=train[train['id']==n_id[0]].iloc[:,1:8] #id==0 데이터\n","ex2=train[train['id']==n_id[0]+3125].iloc[:,1:8] # id==0 증강 데이터\n","ex3=train[train['id']==n_id[0]+6250].iloc[:,1:8]\n","ex4=train[train['id']==n_id[0]+9375].iloc[:,1:8]\n","ex5=train[train['id']==n_id[0]+12500].iloc[:,1:8]\n","ex6=train[train['id']==n_id[0]+15625].iloc[:,1:8]\n","\n","ax[0,0].plot(ex1)\n","ax[0,1].plot(ex2)\n","ax[0,2].plot(ex3)\n","ax[1,0].plot(ex4)\n","ax[1,1].plot(ex5)\n","ax[1,2].plot(ex6)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hUzyQSHbxgr_"},"source":["train.head(603)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KraEbUF0Oz8-"},"source":["train.drop(['id'], axis=1, inplace=True) # id 의 활용은 끝났으니 제거"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r1o4zGQsqwk7"},"source":["# 가속도\n","train['acc_t'] = (train['acc_x'] ** 2) + (train['acc_y'] ** 2) + (train['acc_z'] ** 2) ** (1/3)\n","test['acc_t'] = (test['acc_x'] ** 2) + (test['acc_y'] ** 2) + (test['acc_z'] ** 2) ** (1/3)\n","\n","train['gy_t'] = (train['gy_x'] ** 2) + (train['gy_y'] ** 2) + (train['gy_z'] ** 2) ** (1/3)\n","test['gy_t'] = (test['gy_x'] ** 2) + (test['gy_y'] ** 2) + (test['gy_z'] ** 2) ** (1/3)\n","\n","# Signal 극대화 (peak 캐치 유용)\n","train['acc_mag'] = (train['acc_x'] ** 2) + (train['acc_y'] ** 2) + (train['acc_z'] ** 2)\n","test['acc_mag'] = (test['acc_x'] ** 2) + (test['acc_y'] ** 2) + (test['acc_z'] ** 2)\n","\n","train['gy_mag'] = (train['gy_x'] ** 2) + (train['gy_y'] ** 2) + (train['gy_z'] ** 2)\n","test['gy_mag'] = (test['gy_x'] ** 2) + (test['gy_y'] ** 2) + (test['gy_z'] ** 2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KE_v-XPXBEwJ"},"source":["# vector\n","train['acc_vec'] = np.sqrt((train['acc_x'] ** 2) +(train['acc_y'] ** 2)+(train['acc_z'] ** 2))\n","test['acc_vec'] = np.sqrt((test['acc_x'] ** 2) +(test['acc_y'] ** 2)+(test['acc_z'] ** 2))\n","\n","train['gy_vec'] = np.sqrt((train['gy_x'] ** 2) +(train['gy_y'] ** 2)+(train['gy_z'] ** 2))\n","test['gy_vec'] = np.sqrt((test['gy_x'] ** 2) +(test['gy_y'] ** 2)+(test['gy_z'] ** 2))\n","\n","# 자이로스코프 무게중심\n","train['gy_gravity'] = (train['gy_x']+train['gy_y']+train['gy_z'])/3\n","test['gy_gravity'] = (test['gy_x']+test['gy_y']+test['gy_z'])/3"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U_FgcUVljSip"},"source":["# roll & pitch\n","train['roll'] = np.arctan(train['acc_y']/np.sqrt(train['acc_x'] ** 2 + train['acc_z'] ** 2))\n","test['roll'] = np.arctan(test['acc_y']/np.sqrt(test['acc_x'] ** 2 + test['acc_z'] ** 2))\n","\n","train['pitch'] = np.arctan(train['acc_x']/np.sqrt(train['acc_y'] ** 2 + train['acc_z'] ** 2))\n","test['pitch'] = np.arctan(test['acc_x']/np.sqrt(test['acc_y'] ** 2 + test['acc_z'] ** 2))\n","\n","train['math_roll'] = np.arctan(- train['acc_x']/np.sqrt(train['acc_y'] ** 2 + train['acc_z'] ** 2)) * (180/pi)\n","test['math_roll'] = np.arctan(- test['acc_x']/np.sqrt(test['acc_y'] ** 2 + test['acc_z'] ** 2)) * (180/pi)\n","\n","train['math_pitch'] = np.arctan(train['acc_y']/np.sqrt(train['acc_x'] ** 2 + train['acc_z'] ** 2)) * (180/pi)\n","test['math_pitch'] = np.arctan(test['acc_y']/np.sqrt(test['acc_x'] ** 2 + test['acc_z'] ** 2)) * (180/pi)\n","\n","train['gy_roll'] = np.arctan(train['gy_y']/np.sqrt(train['gy_x'] ** 2 + train['gy_z'] ** 2))\n","test['gy_roll'] = np.arctan(test['gy_y']/np.sqrt(test['gy_x'] ** 2 + test['gy_z'] ** 2))\n","\n","train['gy_pitch'] = np.arctan(train['gy_x']/np.sqrt(train['gy_y'] ** 2 + train['gy_z'] ** 2))\n","test['gy_pitch'] = np.arctan(test['gy_x']/np.sqrt(test['gy_y'] ** 2 + test['gy_z'] ** 2))\n","\n","train['gy_math_roll'] = np.arctan(- train['gy_x']/np.sqrt(train['gy_y'] ** 2 + train['gy_z'] ** 2)) * (180/pi)\n","test['gy_math_roll'] = np.arctan(- test['gy_x']/np.sqrt(test['gy_y'] ** 2 + test['gy_z'] ** 2)) * (180/pi)\n","\n","train['gy_math_pitch'] = np.arctan(train['gy_y']/np.sqrt(train['gy_x'] ** 2 + train['gy_z'] ** 2)) * (180/pi)\n","test['gy_math_pitch'] = np.arctan(test['gy_y']/np.sqrt(test['gy_x'] ** 2 + test['gy_z'] ** 2)) * (180/pi)\n","\n","print(train.shape)\n","train"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KG0UxyQzqy38"},"source":["# Scaling 원하는 걸로 사용\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.preprocessing import RobustScaler\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.preprocessing import Normalizer\n","\n","scaler = RobustScaler()\n","train = scaler.fit_transform(train)\n","test.drop(['id', 'time'], axis=1, inplace=True)\n","test = scaler.transform(test)\n","train"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gqGdCWomjITD"},"source":["import tensorflow as tf \n","from keras.models import Sequential\n","from keras.layers import Dropout, LSTM, Input\n","from keras.layers import TimeDistributed\n","from keras.layers import Activation, GlobalAveragePooling1D\n","from keras.layers import Dense, Flatten, BatchNormalization\n","from keras.layers.convolutional import Conv1D\n","from keras.layers.convolutional import MaxPooling1D\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras import Sequential\n","from tensorflow.keras.models import Model\n","from keras.callbacks import ModelCheckpoint\n","from keras.callbacks import EarlyStopping\n","from tensorflow.keras import regularizers\n","from keras.models import load_model\n","from keras.layers.merge import concatenate"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3G_9MwV3lOCr"},"source":["len_features = train.shape[1] # feature 갯수\n","X = train.reshape(-1, 600, len_features)\n","X.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"L9BwD5mvnD21"},"source":["y = to_categorical(y_train_total) # label 데이터\n","y.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RMA0WAJ6q6_2"},"source":["# scailing 하면서 2차원 배열 형태를 reshape 해줌\n","test_X = test.reshape(-1, 600, len_features)\n","test_X.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WWxrKNusqy3-"},"source":["epochs, batch_size = 40, 64\n","n_features, n_outputs = X.shape[2], y.shape[1]\n","# reshape data into time steps of sub-sequences\n","n_steps, n_length = 6, 100\n","X = X.reshape((X.shape[0], n_steps, n_length, n_features))\n","test_X = test_X.reshape((test_X.shape[0], n_steps, n_length, n_features))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QsJbpKCIjBoc"},"source":["class Models:\n","    # paramter 다양하게 적용\n","    def define_model_0():\n","        model = Sequential()\n","        model.add(TimeDistributed(Conv1D(filters=64, kernel_size=3, activation='relu'), input_shape=(None,n_length,n_features)))\n","        model.add(TimeDistributed(BatchNormalization()))\n","        model.add(TimeDistributed(Conv1D(filters=32, kernel_size=3, activation='relu')))\n","        model.add(TimeDistributed(BatchNormalization()))\n","        model.add(TimeDistributed(Dropout(0.5)))\n","        model.add(TimeDistributed(GlobalAveragePooling1D()))\n","        model.add(LSTM(16))\n","        model.add(Dropout(0.5))\n","        model.add(Dense(128, activation='relu'))\n","        model.add(Dense(n_outputs, activation='softmax'))\n","        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","        \n","        return model\n","\n","    def define_model_1():\n","        model = Sequential()\n","        model.add(TimeDistributed(Conv1D(filters=64, kernel_size=3, activation='relu'), input_shape=(None,n_length,n_features)))\n","        model.add(TimeDistributed(Dropout(0.5)))\n","        model.add(TimeDistributed(GlobalAveragePooling1D()))\n","        model.add(LSTM(32))\n","        model.add(Dropout(0.5))\n","        model.add(Dense(128, activation='relu'))\n","        model.add(Dense(n_outputs, activation='softmax'))\n","        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","        \n","        return model\n","\n","    def define_model_2():\n","        model = Sequential()\n","        model.add(TimeDistributed(Conv1D(filters=64, kernel_size=3, activation='relu'), input_shape=(None,n_length,n_features)))\n","        model.add(TimeDistributed(BatchNormalization()))\n","        model.add(TimeDistributed(Conv1D(filters=64, kernel_size=3, activation='relu')))\n","        model.add(TimeDistributed(Dropout(0.5)))\n","        model.add(TimeDistributed(GlobalAveragePooling1D()))\n","        model.add(LSTM(16))\n","        model.add(Dropout(0.5))\n","        model.add(Dense(100, activation='relu'))\n","        model.add(Dense(n_outputs, activation='softmax'))\n","        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","        \n","        return model\n","\n","    def define_model_3():\n","        model = Sequential()\n","        model.add(TimeDistributed(Conv1D(filters=64, kernel_size=9, activation='relu'), input_shape=(None,n_length,n_features)))\n","        model.add(TimeDistributed(BatchNormalization()))\n","        model.add(TimeDistributed(Conv1D(filters=32, kernel_size=6, activation='relu')))\n","        model.add(TimeDistributed(BatchNormalization()))\n","        model.add(TimeDistributed(Dropout(0.5)))\n","        model.add(TimeDistributed(Conv1D(filters=32, kernel_size=3, activation='relu')))\n","        model.add(TimeDistributed(BatchNormalization()))\n","        model.add(TimeDistributed(Dropout(0.5)))\n","        model.add(TimeDistributed(GlobalAveragePooling1D()))\n","        model.add(LSTM(16))\n","        model.add(Dropout(0.5))\n","        model.add(Dense(128, activation='relu'))\n","        model.add(Dense(n_outputs, activation='softmax'))\n","        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","        \n","        return model\n","\n","    def define_model_4():\n","        model = Sequential()\n","        model.add(TimeDistributed(Conv1D(filters=64, kernel_size=9, activation='relu'), input_shape=(None,n_length,n_features)))\n","        model.add(TimeDistributed(BatchNormalization()))\n","        model.add(TimeDistributed(Conv1D(filters=64, kernel_size=6, activation='relu')))\n","        model.add(TimeDistributed(BatchNormalization()))\n","        model.add(TimeDistributed(Dropout(0.5)))\n","        model.add(TimeDistributed(Conv1D(filters=64, kernel_size=3, activation='relu')))\n","        model.add(TimeDistributed(BatchNormalization()))\n","        model.add(TimeDistributed(Dropout(0.5)))\n","        model.add(TimeDistributed(GlobalAveragePooling1D()))\n","        model.add(LSTM(32))\n","        model.add(Dropout(0.5))\n","        model.add(Dense(128, activation='relu'))\n","        model.add(Dense(n_outputs, activation='softmax'))\n","        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","        \n","        return model\n","\n","    def define_model_5():\n","        model = Sequential()\n","        model.add(TimeDistributed(Conv1D(filters=32, kernel_size=9, activation='relu'), input_shape=(None,n_length,n_features)))\n","        model.add(TimeDistributed(BatchNormalization()))\n","        model.add(TimeDistributed(Conv1D(filters=32, kernel_size=6, activation='relu')))\n","        model.add(TimeDistributed(BatchNormalization()))\n","        model.add(TimeDistributed(Dropout(0.5)))\n","        model.add(TimeDistributed(Conv1D(filters=32, kernel_size=3, activation='relu')))\n","        model.add(TimeDistributed(BatchNormalization()))\n","        model.add(TimeDistributed(Dropout(0.5)))\n","        model.add(TimeDistributed(GlobalAveragePooling1D()))\n","        model.add(LSTM(32))\n","        model.add(Dropout(0.5))\n","        model.add(Dense(128, activation='relu'))\n","        model.add(Dense(n_outputs, activation='softmax'))\n","        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","        \n","        return model\n","\n","    def define_model_6():\n","        model = Sequential()\n","        model.add(TimeDistributed(Conv1D(filters=64, kernel_size=6, activation='relu'), input_shape=(None,n_length,n_features)))\n","        model.add(TimeDistributed(BatchNormalization()))\n","        model.add(TimeDistributed(Conv1D(filters=64, kernel_size=6, activation='relu')))\n","        model.add(TimeDistributed(BatchNormalization()))\n","        model.add(TimeDistributed(Dropout(0.5)))\n","        model.add(TimeDistributed(Conv1D(filters=64, kernel_size=3, activation='relu')))\n","        model.add(TimeDistributed(BatchNormalization()))\n","        model.add(TimeDistributed(Dropout(0.5)))\n","        model.add(TimeDistributed(GlobalAveragePooling1D()))\n","        model.add(LSTM(32))\n","        model.add(Dropout(0.5))\n","        model.add(Dense(128, activation='relu'))\n","        model.add(Dense(n_outputs, activation='softmax'))\n","        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","        \n","        return model\n","\n","    def define_model_7():\n","        model = Sequential()\n","        model.add(TimeDistributed(Conv1D(filters=64, kernel_size=6, activation='relu'), input_shape=(None,n_length,n_features)))\n","        model.add(TimeDistributed(BatchNormalization()))\n","        model.add(TimeDistributed(Conv1D(filters=64, kernel_size=6, activation='relu')))\n","        model.add(TimeDistributed(BatchNormalization()))\n","        model.add(TimeDistributed(Dropout(0.5)))\n","        model.add(TimeDistributed(GlobalAveragePooling1D()))\n","        model.add(LSTM(32))\n","        model.add(Dropout(0.5))\n","        model.add(Dense(128, activation='relu'))\n","        model.add(Dense(n_outputs, activation='softmax'))\n","        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","        \n","        return model\n","\n","    def define_model_8():\n","        model = Sequential()\n","        model.add(TimeDistributed(Conv1D(filters=64, kernel_size=3, activation='relu'), input_shape=(None,n_length,n_features)))\n","        model.add(TimeDistributed(BatchNormalization()))\n","        model.add(TimeDistributed(Conv1D(filters=64, kernel_size=3, activation='relu')))\n","        model.add(TimeDistributed(BatchNormalization()))\n","        model.add(TimeDistributed(Dropout(0.5)))\n","        model.add(TimeDistributed(GlobalAveragePooling1D()))\n","        model.add(LSTM(32))\n","        model.add(Dropout(0.5))\n","        model.add(Dense(128, activation='relu'))\n","        model.add(Dense(n_outputs, activation='softmax'))\n","        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","        \n","        return model\n","\n","    def define_model_9():\n","        model = Sequential()\n","        model.add(TimeDistributed(Conv1D(filters=128, kernel_size=3, activation='relu'), input_shape=(None,n_length,n_features)))\n","        model.add(TimeDistributed(BatchNormalization()))\n","        model.add(TimeDistributed(Conv1D(filters=64, kernel_size=6, activation='relu')))\n","        model.add(TimeDistributed(BatchNormalization()))\n","        model.add(TimeDistributed(Dropout(0.5)))\n","        model.add(TimeDistributed(GlobalAveragePooling1D()))\n","        model.add(LSTM(32))\n","        model.add(Dropout(0.5))\n","        model.add(Dense(128, activation='relu'))\n","        model.add(Dense(n_outputs, activation='softmax'))\n","        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","        \n","        return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pvJ-AalijBrV"},"source":["# 위의 모델들 학습하면서 stacking\n","for i in range(10): # 모델 수 만큼 반복\n","    model = getattr(Models, f'define_model_{i}')() # 모델 불러오기\n","    checkpoint_path = \"checkpoint/cp.ckpt\" # checkpoint 설정\n","    cp_callback = ModelCheckpoint(filepath=checkpoint_path, monitor='loss', \n","                                verbose=1, save_weights_only=True, \n","                                save_best_only=True, mode='min')\n","    early_stopping = EarlyStopping(monitor='loss', patience=10, mode='min')\n","    model.fit(X, y, epochs=epochs, batch_size=batch_size, \n","            validation_split=0.2, callbacks=[early_stopping, cp_callback])\n","    model.save(f'models/model_{i}.h5') # 학습시킨 모델 저장\n","    tf.keras.backend.clear_session()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-Sqxkt_QkEUa"},"source":["# 저장한 모델 불러오기\n","for i in range(10): # 모델 갯수\n","    globals()[f'model{i}'] = load_model(f'models/model_{i}.h5')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t4TQvwEz73Kg"},"source":["# 독립적인 모델 이름 설정 (모델 수 만큼 필요)\n","# 기본 10개 필요없으면 주석처리해서 사용\n","model0._name = 'Client0'\n","model1._name = 'Client1'\n","model2._name = 'Client2'\n","model3._name = 'Client3'\n","model4._name = 'Client4'\n","model5._name = 'Client5'\n","model6._name = 'Client6'\n","model7._name = 'Client7'\n","model8._name = 'Client8'\n","model9._name = 'Client9'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zMyFk1x-kEYe"},"source":["inputs = Input(shape=(n_steps, n_length, n_features))\n","\n","# 모델 합치기\n","merge = concatenate([model0(inputs), model1(inputs), model2(inputs), \n","                     model3(inputs), model4(inputs), model5(inputs),\n","                     model6(inputs), model7(inputs), model8(inputs),\n","                     model9(inputs)])\n","hidden = Dense(10, activation='relu')(merge)\n","output = Dense(61, activation='softmax')(hidden)\n","model = tf.keras.models.Model(inputs=inputs, outputs=output)\n","\n","model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MzojcfSAcGWO"},"source":["# 교차 검증 Cross validation\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.metrics import accuracy_score\n","\n","stfold = StratifiedKFold(n_splits=3, shuffle=True)\n","idx_iter = 0 \n","skf_accuracy=[]\n","\n","for train_idx, valid_idx in stfold.split(X, y_train_total) : \n","    Y_train, Y_valid = tf.gather(y, train_idx), tf.gather(y, valid_idx)\n","    X_train, X_valid = tf.gather(X, train_idx), tf.gather(X, valid_idx)\n","\n","    checkpoint_path = \"checkpoint/cp2.ckpt\"\n","    cp_callback = ModelCheckpoint(filepath=checkpoint_path, monitor='val_loss', verbose=1, save_weights_only=True, save_best_only=True, mode='min')\n","\n","    early_stopping = EarlyStopping(monitor='loss', patience=4, mode='min')\n","    model.fit(X_train, Y_train, epochs=30, batch_size=batch_size, callbacks=[early_stopping, cp_callback])\n","    pred = model.predict(X_valid)\n","\n","    # 반복 시 마다 정확도 측정\n","    idx_iter += 1 \n","    y_pred = (pred > 0.5) \n","    accuracy = np.round(accuracy_score(Y_valid, y_pred), 4)\n","    train_size = X_train.shape[0]\n","    test_size = X_valid.shape[0]\n","\n","    print(\"\\n##### 교차 검증: {}, 정확도: {}  #####\" .format(idx_iter, accuracy))\n","    print('학습 레이블 데이터 분포:\\n ', Y_train.shape[0])\n","    print('검증 레이블 데이터 분포:\\n ', Y_valid.shape[0], '\\n\\n')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rUDiw-fA05uf"},"source":["prediction = model.predict(test_X)\n","prediction.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fP8wkIVHL79U"},"source":["submission"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UvIniXVjtAkG"},"source":["prediction"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ngJyjpIBL972"},"source":["submission.iloc[:,1:]=prediction"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ABb6IgKAL_9N"},"source":["submission"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y1ir4C2oMBFS"},"source":["submission.to_csv('submission/sub_last_5.csv', index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"R8VVMoABCXxq"},"source":[""],"execution_count":null,"outputs":[]}]}