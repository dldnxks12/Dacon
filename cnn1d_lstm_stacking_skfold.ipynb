{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "hide_input": false,
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.10 64-bit ('ds': venv)"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "name": "cnn1d_lstm_stacking.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU",
    "interpreter": {
      "hash": "172d357f1c92d4c08eada0d83cc0b4d7a3e15628b28e34b7092f9013b5b73d18"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sm0xZ2uXk6Xg",
        "outputId": "f2ff1378-bd91-46cd-8566-1858a275c6e2"
      },
      "source": [
        "# 구글 드라이브 마운트\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmIJ4F3Lk6Il"
      },
      "source": [
        "# 기본 directory 설정\n",
        "import os\n",
        "os.chdir('/content/drive/MyDrive/Colab Notebooks/AI hackathon/data/')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6d7M76Wl5Bx"
      },
      "source": [
        "# 모듈 불러오기\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from math import pi"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFXySamQk5y-"
      },
      "source": [
        "# 데이터 불러오기\n",
        "path = './' # 기본 directory 경로에 추가 할 경로\n",
        "\n",
        "train = pd.read_csv(path + 'train_features.csv')\n",
        "train_labels = pd.read_csv(path + 'train_labels.csv')\n",
        "test = pd.read_csv(path + 'test_features.csv')\n",
        "submission = pd.read_csv(path + 'sample_submission.csv')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7Ncehkzk22p",
        "outputId": "a68019b1-4f9e-4031-fa50-25890cfac408"
      },
      "source": [
        "act_list=train.iloc[:,2:].columns\n",
        "acc_list=['acc_x','acc_y','acc_z']\n",
        "gy_list=['gy_x','gy_y','gy_z']\n",
        "act_list"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['acc_x', 'acc_y', 'acc_z', 'gy_x', 'gy_y', 'gy_z'], dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sK8gCKp3lp3D"
      },
      "source": [
        "# acc 데이터와 gy 데이터로 분할\n",
        "def sensor_split(data):\n",
        "    X_acc = []\n",
        "    X_gy = []\n",
        "\n",
        "    for i in tqdm(data['id'].unique()):\n",
        "        temp_acc = np.array(data[data['id'] == i].loc[:,acc_list])\n",
        "        temp_gy = np.array(data[data['id'] == i].loc[:,gy_list])\n",
        "        X_acc.append(temp_acc)\n",
        "        X_gy.append(temp_gy)\n",
        "      \n",
        "    X_acc = np.array(X_acc).reshape(-1,600,3)\n",
        "    X_gy = np.array(X_gy).reshape(-1,600,3)\n",
        "\n",
        "    return X_acc, X_gy"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6p-mjEZaltsK"
      },
      "source": [
        "# 데이터 증강\n",
        "\n",
        "def permutation(X, nPerm=4, minSegLength=10):\n",
        "    X_new = np.zeros(X.shape)\n",
        "    idx = np.random.permutation(nPerm)\n",
        "    bWhile = True\n",
        "    while bWhile == True:\n",
        "        segs = np.zeros(nPerm+1, dtype=int)\n",
        "        segs[1:-1] = np.sort(np.random.randint(minSegLength, X.shape[0]-minSegLength, nPerm-1))\n",
        "        segs[-1] = X.shape[0]\n",
        "        if np.min(segs[1:]-segs[0:-1]) > minSegLength:\n",
        "            bWhile = False\n",
        "    pp = 0\n",
        "    for ii in range(nPerm):\n",
        "        x_temp = X[segs[idx[ii]]:segs[idx[ii]+1],:]\n",
        "        X_new[pp:pp+len(x_temp),:] = x_temp\n",
        "        pp += len(x_temp)\n",
        "    return (X_new)\n",
        "\n",
        "def aug(data, uid, shift):\n",
        "    shift_data = np.roll(data[uid], shift, axis=0)\n",
        "    return shift_data\n",
        "def rolling(data):\n",
        "    aug_data=[]\n",
        "    for i in range(data.shape[0]):\n",
        "        temp=list((aug(data,i,int(random.random()*600))))\n",
        "        aug_data.append(temp)\n",
        "    return np.array(aug_data)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HECv9WwUTQgZ"
      },
      "source": [
        "# 데이터 증강 (반복하고 싶은 만큼 조정)\n",
        "def start_augmentation(train, train_labels):\n",
        "    # acc, gy 데이터 분할\n",
        "    X_train_mod=pd.merge(train,train_labels,how='left',on='id')\n",
        "    X_train_acc, X_train_gy= sensor_split(X_train_mod)\n",
        "\n",
        "    # 증강시키고 추가할 임시 데이터 복사본\n",
        "    X_train_acc_temp = X_train_acc.copy()\n",
        "    X_train_gy_temp = X_train_gy.copy()\n",
        "\n",
        "    # label 데이터 변환\n",
        "    y_train = train_labels['label']\n",
        "    y_train_total = np.append(y_train, y_train, axis=0)\n",
        "\n",
        "    rep = 3 # 5이상의 경우 reshape 과정에서 reset될 가능성 높음\n",
        "    for i in range(rep):\n",
        "        X_train_acc_roll = rolling(X_train_acc_temp)\n",
        "        X_train_acc_rp = permutation(rolling(X_train_acc_temp)) # rolling + permutation\n",
        "\n",
        "        X_train_gy_roll = rolling(X_train_gy_temp)\n",
        "        X_train_gy_rp = permutation(rolling(X_train_gy_temp)) # rolling + permutation\n",
        "\n",
        "        # 증강시킨 데이터 원래 데이터에 추가\n",
        "        X_train_acc = np.append(X_train_acc, X_train_acc_roll, axis=0)\n",
        "        X_train_acc = np.append(X_train_acc, X_train_acc_rp, axis=0)\n",
        "\n",
        "        X_train_gy = np.append(X_train_gy, X_train_gy_roll, axis=0)\n",
        "        X_train_gy = np.append(X_train_gy, X_train_gy_rp, axis=0)\n",
        "\n",
        "        y_train_total = np.append(y_train_total, y_train, axis=0)\n",
        "        if i != (rep-1): # 마지막 한 번 제외\n",
        "            y_train_total = np.append(y_train_total, y_train, axis=0)\n",
        "\n",
        "    return X_train_acc, X_train_gy, y_train_total "
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pwbF7XSAyy4l",
        "outputId": "f819fba8-4e59-4431-f62d-fc3cf73fadc8"
      },
      "source": [
        "X_train_acc, X_train_gy, y_train_total = start_augmentation(train, train_labels)\n",
        "\n",
        "X_train_acc.shape, X_train_gy.shape, y_train_total.shape"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3125/3125 [00:23<00:00, 132.68it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((21875, 600, 3), (21875, 600, 3), (21875,))"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_jVLRgKnGeU"
      },
      "source": [
        "# np array 형태를 dataframe 으로 변환\n",
        "def np_to_df(X_train_acc, X_train_gy):\n",
        "    acc = [e for sl in X_train_acc for e in sl]\n",
        "    gy = [e for sl in X_train_gy for e in sl]\n",
        "\n",
        "    df_report_acc = np.stack(acc, axis = 0)\n",
        "    df_report_gy = np.stack(gy, axis = 0)\n",
        "\n",
        "    df_acc = pd.DataFrame(df_report_acc, columns= ['acc_x', 'acc_y', 'acc_z']) \n",
        "    df_gy = pd.DataFrame(df_report_gy, columns= ['gy_x', 'gy_y', 'gy_z']) \n",
        "\n",
        "    # acc, gy 데이터프레임 병합\n",
        "    df_aug_result = pd.concat([df_acc, df_gy], axis = 1)\n",
        "    \n",
        "    return df_aug_result"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "T0qgbFfRnO_g",
        "outputId": "b75d6991-dbbc-422d-e7e4-5487fc91c4af"
      },
      "source": [
        "train = np_to_df(X_train_acc, X_train_gy)\n",
        "train"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>acc_x</th>\n",
              "      <th>acc_y</th>\n",
              "      <th>acc_z</th>\n",
              "      <th>gy_x</th>\n",
              "      <th>gy_y</th>\n",
              "      <th>gy_z</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.206087</td>\n",
              "      <td>-0.179371</td>\n",
              "      <td>-0.148447</td>\n",
              "      <td>-0.591608</td>\n",
              "      <td>-30.549010</td>\n",
              "      <td>-31.676112</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.287696</td>\n",
              "      <td>-0.198974</td>\n",
              "      <td>-0.182444</td>\n",
              "      <td>0.303100</td>\n",
              "      <td>-39.139103</td>\n",
              "      <td>-24.927216</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.304609</td>\n",
              "      <td>-0.195114</td>\n",
              "      <td>-0.253382</td>\n",
              "      <td>-3.617278</td>\n",
              "      <td>-44.122565</td>\n",
              "      <td>-25.019629</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.293095</td>\n",
              "      <td>-0.230366</td>\n",
              "      <td>-0.215210</td>\n",
              "      <td>2.712986</td>\n",
              "      <td>-53.597843</td>\n",
              "      <td>-27.454013</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.300887</td>\n",
              "      <td>-0.187757</td>\n",
              "      <td>-0.222523</td>\n",
              "      <td>4.286707</td>\n",
              "      <td>-57.906561</td>\n",
              "      <td>-27.961234</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13124995</th>\n",
              "      <td>-0.028493</td>\n",
              "      <td>-0.994816</td>\n",
              "      <td>0.364849</td>\n",
              "      <td>-6.710098</td>\n",
              "      <td>-95.525230</td>\n",
              "      <td>-83.847600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13124996</th>\n",
              "      <td>-0.031233</td>\n",
              "      <td>-1.023963</td>\n",
              "      <td>0.342354</td>\n",
              "      <td>-7.480734</td>\n",
              "      <td>-94.221777</td>\n",
              "      <td>-84.194322</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13124997</th>\n",
              "      <td>-0.044125</td>\n",
              "      <td>-0.953804</td>\n",
              "      <td>0.386487</td>\n",
              "      <td>-7.768990</td>\n",
              "      <td>-93.280180</td>\n",
              "      <td>-84.567775</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13124998</th>\n",
              "      <td>-0.041707</td>\n",
              "      <td>-0.868214</td>\n",
              "      <td>0.404388</td>\n",
              "      <td>-8.564871</td>\n",
              "      <td>-92.980745</td>\n",
              "      <td>-85.093899</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13124999</th>\n",
              "      <td>-0.014338</td>\n",
              "      <td>-0.853726</td>\n",
              "      <td>0.384025</td>\n",
              "      <td>-8.939372</td>\n",
              "      <td>-92.541060</td>\n",
              "      <td>-85.111657</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>13125000 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "             acc_x     acc_y     acc_z      gy_x       gy_y       gy_z\n",
              "0         1.206087 -0.179371 -0.148447 -0.591608 -30.549010 -31.676112\n",
              "1         1.287696 -0.198974 -0.182444  0.303100 -39.139103 -24.927216\n",
              "2         1.304609 -0.195114 -0.253382 -3.617278 -44.122565 -25.019629\n",
              "3         1.293095 -0.230366 -0.215210  2.712986 -53.597843 -27.454013\n",
              "4         1.300887 -0.187757 -0.222523  4.286707 -57.906561 -27.961234\n",
              "...            ...       ...       ...       ...        ...        ...\n",
              "13124995 -0.028493 -0.994816  0.364849 -6.710098 -95.525230 -83.847600\n",
              "13124996 -0.031233 -1.023963  0.342354 -7.480734 -94.221777 -84.194322\n",
              "13124997 -0.044125 -0.953804  0.386487 -7.768990 -93.280180 -84.567775\n",
              "13124998 -0.041707 -0.868214  0.404388 -8.564871 -92.980745 -85.093899\n",
              "13124999 -0.014338 -0.853726  0.384025 -8.939372 -92.541060 -85.111657\n",
              "\n",
              "[13125000 rows x 6 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1o4zGQsqwk7"
      },
      "source": [
        "# 가속도\n",
        "train['acc_t'] = (train['acc_x'] ** 2) + (train['acc_y'] ** 2) + (train['acc_z'] ** 2) ** (1/3)\n",
        "test['acc_t'] = (test['acc_x'] ** 2) + (test['acc_y'] ** 2) + (test['acc_z'] ** 2) ** (1/3)\n",
        "\n",
        "train['gy_t'] = (train['gy_x'] ** 2) + (train['gy_y'] ** 2) + (train['gy_z'] ** 2) ** (1/3)\n",
        "test['gy_t'] = (test['gy_x'] ** 2) + (test['gy_y'] ** 2) + (test['gy_z'] ** 2) ** (1/3)\n",
        "\n",
        "# Signal 극대화 (peak 캐치 유용)\n",
        "train['acc_mag'] = (train['acc_x'] ** 2) + (train['acc_y'] ** 2) + (train['acc_z'] ** 2)\n",
        "test['acc_mag'] = (test['acc_x'] ** 2) + (test['acc_y'] ** 2) + (test['acc_z'] ** 2)\n",
        "\n",
        "train['gy_mag'] = (train['gy_x'] ** 2) + (train['gy_y'] ** 2) + (train['gy_z'] ** 2)\n",
        "test['gy_mag'] = (test['gy_x'] ** 2) + (test['gy_y'] ** 2) + (test['gy_z'] ** 2)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KE_v-XPXBEwJ"
      },
      "source": [
        "# vector\n",
        "train['acc_vec'] = np.sqrt((train['acc_x'] ** 2) +(train['acc_y'] ** 2)+(train['acc_z'] ** 2))\n",
        "test['acc_vec'] = np.sqrt((test['acc_x'] ** 2) +(test['acc_y'] ** 2)+(test['acc_z'] ** 2))\n",
        "\n",
        "train['gy_vec'] = np.sqrt((train['gy_x'] ** 2) +(train['gy_y'] ** 2)+(train['gy_z'] ** 2))\n",
        "test['gy_vec'] = np.sqrt((test['gy_x'] ** 2) +(test['gy_y'] ** 2)+(test['gy_z'] ** 2))\n",
        "\n",
        "# 자이로스코프 무게중심\n",
        "train['gy_gravity'] = (train['gy_x']+train['gy_y']+train['gy_z'])/3\n",
        "test['gy_gravity'] = (test['gy_x']+test['gy_y']+test['gy_z'])/3"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 457
        },
        "id": "U_FgcUVljSip",
        "outputId": "7479d8d4-1a2f-4d54-92c9-dfdd9679beeb"
      },
      "source": [
        "# roll & pitch\n",
        "train['roll'] = np.arctan(train['acc_y']/np.sqrt(train['acc_x'] ** 2 + train['acc_z'] ** 2))\n",
        "test['roll'] = np.arctan(test['acc_y']/np.sqrt(test['acc_x'] ** 2 + test['acc_z'] ** 2))\n",
        "\n",
        "train['pitch'] = np.arctan(train['acc_x']/np.sqrt(train['acc_y'] ** 2 + train['acc_z'] ** 2))\n",
        "test['pitch'] = np.arctan(test['acc_x']/np.sqrt(test['acc_y'] ** 2 + test['acc_z'] ** 2))\n",
        "\n",
        "train['math_roll'] = np.arctan(- train['acc_x']/np.sqrt(train['acc_y'] ** 2 + train['acc_z'] ** 2)) * (180/pi)\n",
        "test['math_roll'] = np.arctan(- test['acc_x']/np.sqrt(test['acc_y'] ** 2 + test['acc_z'] ** 2)) * (180/pi)\n",
        "\n",
        "train['math_pitch'] = np.arctan(train['acc_y']/np.sqrt(train['acc_x'] ** 2 + train['acc_z'] ** 2)) * (180/pi)\n",
        "test['math_pitch'] = np.arctan(test['acc_y']/np.sqrt(test['acc_x'] ** 2 + test['acc_z'] ** 2)) * (180/pi)\n",
        "\n",
        "train['gy_roll'] = np.arctan(train['gy_y']/np.sqrt(train['gy_x'] ** 2 + train['gy_z'] ** 2))\n",
        "test['gy_roll'] = np.arctan(test['gy_y']/np.sqrt(test['gy_x'] ** 2 + test['gy_z'] ** 2))\n",
        "\n",
        "train['gy_pitch'] = np.arctan(train['gy_x']/np.sqrt(train['gy_y'] ** 2 + train['gy_z'] ** 2))\n",
        "test['gy_pitch'] = np.arctan(test['gy_x']/np.sqrt(test['gy_y'] ** 2 + test['gy_z'] ** 2))\n",
        "\n",
        "train['gy_math_roll'] = np.arctan(- train['gy_x']/np.sqrt(train['gy_y'] ** 2 + train['gy_z'] ** 2)) * (180/pi)\n",
        "test['gy_math_roll'] = np.arctan(- test['gy_x']/np.sqrt(test['gy_y'] ** 2 + test['gy_z'] ** 2)) * (180/pi)\n",
        "\n",
        "train['gy_math_pitch'] = np.arctan(train['gy_y']/np.sqrt(train['gy_x'] ** 2 + train['gy_z'] ** 2)) * (180/pi)\n",
        "test['gy_math_pitch'] = np.arctan(test['gy_y']/np.sqrt(test['gy_x'] ** 2 + test['gy_z'] ** 2)) * (180/pi)\n",
        "\n",
        "print(train.shape)\n",
        "train"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(13125000, 21)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>acc_x</th>\n",
              "      <th>acc_y</th>\n",
              "      <th>acc_z</th>\n",
              "      <th>gy_x</th>\n",
              "      <th>gy_y</th>\n",
              "      <th>gy_z</th>\n",
              "      <th>acc_t</th>\n",
              "      <th>gy_t</th>\n",
              "      <th>acc_mag</th>\n",
              "      <th>gy_mag</th>\n",
              "      <th>acc_vec</th>\n",
              "      <th>gy_vec</th>\n",
              "      <th>gy_gravity</th>\n",
              "      <th>roll</th>\n",
              "      <th>pitch</th>\n",
              "      <th>math_roll</th>\n",
              "      <th>math_pitch</th>\n",
              "      <th>gy_roll</th>\n",
              "      <th>gy_pitch</th>\n",
              "      <th>gy_math_roll</th>\n",
              "      <th>gy_math_pitch</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.206087</td>\n",
              "      <td>-0.179371</td>\n",
              "      <td>-0.148447</td>\n",
              "      <td>-0.591608</td>\n",
              "      <td>-30.549010</td>\n",
              "      <td>-31.676112</td>\n",
              "      <td>1.767177</td>\n",
              "      <td>943.603223</td>\n",
              "      <td>1.508855</td>\n",
              "      <td>1936.968046</td>\n",
              "      <td>1.228355</td>\n",
              "      <td>44.010999</td>\n",
              "      <td>-20.938910</td>\n",
              "      <td>-0.146550</td>\n",
              "      <td>1.380095</td>\n",
              "      <td>-79.073624</td>\n",
              "      <td>-8.396683</td>\n",
              "      <td>-0.767200</td>\n",
              "      <td>-0.013443</td>\n",
              "      <td>0.770209</td>\n",
              "      <td>-43.957305</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.287696</td>\n",
              "      <td>-0.198974</td>\n",
              "      <td>-0.182444</td>\n",
              "      <td>0.303100</td>\n",
              "      <td>-39.139103</td>\n",
              "      <td>-24.927216</td>\n",
              "      <td>2.019429</td>\n",
              "      <td>1540.494535</td>\n",
              "      <td>1.731038</td>\n",
              "      <td>2153.327355</td>\n",
              "      <td>1.315689</td>\n",
              "      <td>46.403958</td>\n",
              "      <td>-21.254406</td>\n",
              "      <td>-0.151814</td>\n",
              "      <td>1.364146</td>\n",
              "      <td>-78.159835</td>\n",
              "      <td>-8.698302</td>\n",
              "      <td>-1.003661</td>\n",
              "      <td>0.006532</td>\n",
              "      <td>-0.374246</td>\n",
              "      <td>-57.505519</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.304609</td>\n",
              "      <td>-0.195114</td>\n",
              "      <td>-0.253382</td>\n",
              "      <td>-3.617278</td>\n",
              "      <td>-44.122565</td>\n",
              "      <td>-25.019629</td>\n",
              "      <td>2.140496</td>\n",
              "      <td>1968.439749</td>\n",
              "      <td>1.804277</td>\n",
              "      <td>2585.867212</td>\n",
              "      <td>1.343234</td>\n",
              "      <td>50.851423</td>\n",
              "      <td>-24.253157</td>\n",
              "      <td>-0.145773</td>\n",
              "      <td>1.330405</td>\n",
              "      <td>-76.226605</td>\n",
              "      <td>-8.352177</td>\n",
              "      <td>-1.050508</td>\n",
              "      <td>-0.071194</td>\n",
              "      <td>4.079137</td>\n",
              "      <td>-60.189701</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.293095</td>\n",
              "      <td>-0.230366</td>\n",
              "      <td>-0.215210</td>\n",
              "      <td>2.712986</td>\n",
              "      <td>-53.597843</td>\n",
              "      <td>-27.454013</td>\n",
              "      <td>2.084285</td>\n",
              "      <td>2889.189647</td>\n",
              "      <td>1.771478</td>\n",
              "      <td>3633.811839</td>\n",
              "      <td>1.330969</td>\n",
              "      <td>60.281107</td>\n",
              "      <td>-26.112956</td>\n",
              "      <td>-0.173958</td>\n",
              "      <td>1.331665</td>\n",
              "      <td>-76.298763</td>\n",
              "      <td>-9.967037</td>\n",
              "      <td>-1.095444</td>\n",
              "      <td>0.045021</td>\n",
              "      <td>-2.579501</td>\n",
              "      <td>-62.764338</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.300887</td>\n",
              "      <td>-0.187757</td>\n",
              "      <td>-0.222523</td>\n",
              "      <td>4.286707</td>\n",
              "      <td>-57.906561</td>\n",
              "      <td>-27.961234</td>\n",
              "      <td>2.094771</td>\n",
              "      <td>3380.757973</td>\n",
              "      <td>1.777076</td>\n",
              "      <td>4153.376248</td>\n",
              "      <td>1.333070</td>\n",
              "      <td>64.446693</td>\n",
              "      <td>-27.193696</td>\n",
              "      <td>-0.141316</td>\n",
              "      <td>1.350615</td>\n",
              "      <td>-77.384519</td>\n",
              "      <td>-8.096783</td>\n",
              "      <td>-1.116383</td>\n",
              "      <td>0.066565</td>\n",
              "      <td>-3.813876</td>\n",
              "      <td>-63.964038</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13124995</th>\n",
              "      <td>-0.028493</td>\n",
              "      <td>-0.994816</td>\n",
              "      <td>0.364849</td>\n",
              "      <td>-6.710098</td>\n",
              "      <td>-95.525230</td>\n",
              "      <td>-83.847600</td>\n",
              "      <td>1.501064</td>\n",
              "      <td>9189.252044</td>\n",
              "      <td>1.123585</td>\n",
              "      <td>16200.515121</td>\n",
              "      <td>1.059993</td>\n",
              "      <td>127.281244</td>\n",
              "      <td>-62.027643</td>\n",
              "      <td>-1.218294</td>\n",
              "      <td>-0.026884</td>\n",
              "      <td>1.540313</td>\n",
              "      <td>-69.803098</td>\n",
              "      <td>-0.848826</td>\n",
              "      <td>-0.052743</td>\n",
              "      <td>3.021958</td>\n",
              "      <td>-48.634155</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13124996</th>\n",
              "      <td>-0.031233</td>\n",
              "      <td>-1.023963</td>\n",
              "      <td>0.342354</td>\n",
              "      <td>-7.480734</td>\n",
              "      <td>-94.221777</td>\n",
              "      <td>-84.194322</td>\n",
              "      <td>1.538860</td>\n",
              "      <td>8952.914403</td>\n",
              "      <td>1.166682</td>\n",
              "      <td>16022.388483</td>\n",
              "      <td>1.080131</td>\n",
              "      <td>126.579574</td>\n",
              "      <td>-61.965611</td>\n",
              "      <td>-1.246890</td>\n",
              "      <td>-0.028920</td>\n",
              "      <td>1.656967</td>\n",
              "      <td>-71.441518</td>\n",
              "      <td>-0.839588</td>\n",
              "      <td>-0.059134</td>\n",
              "      <td>3.388101</td>\n",
              "      <td>-48.104839</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13124997</th>\n",
              "      <td>-0.044125</td>\n",
              "      <td>-0.953804</td>\n",
              "      <td>0.386487</td>\n",
              "      <td>-7.768990</td>\n",
              "      <td>-93.280180</td>\n",
              "      <td>-84.567775</td>\n",
              "      <td>1.442276</td>\n",
              "      <td>8780.815667</td>\n",
              "      <td>1.061061</td>\n",
              "      <td>15913.257787</td>\n",
              "      <td>1.030078</td>\n",
              "      <td>126.147762</td>\n",
              "      <td>-61.872315</td>\n",
              "      <td>-1.183551</td>\n",
              "      <td>-0.042850</td>\n",
              "      <td>2.455113</td>\n",
              "      <td>-67.812475</td>\n",
              "      <td>-0.832256</td>\n",
              "      <td>-0.061625</td>\n",
              "      <td>3.530877</td>\n",
              "      <td>-47.684732</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13124998</th>\n",
              "      <td>-0.041707</td>\n",
              "      <td>-0.868214</td>\n",
              "      <td>0.404388</td>\n",
              "      <td>-8.564871</td>\n",
              "      <td>-92.980745</td>\n",
              "      <td>-85.093899</td>\n",
              "      <td>1.302381</td>\n",
              "      <td>8738.122235</td>\n",
              "      <td>0.919064</td>\n",
              "      <td>15959.747519</td>\n",
              "      <td>0.958678</td>\n",
              "      <td>126.331894</td>\n",
              "      <td>-62.213172</td>\n",
              "      <td>-1.132877</td>\n",
              "      <td>-0.043518</td>\n",
              "      <td>2.493416</td>\n",
              "      <td>-64.909095</td>\n",
              "      <td>-0.827148</td>\n",
              "      <td>-0.067849</td>\n",
              "      <td>3.887440</td>\n",
              "      <td>-47.392098</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13124999</th>\n",
              "      <td>-0.014338</td>\n",
              "      <td>-0.853726</td>\n",
              "      <td>0.384025</td>\n",
              "      <td>-8.939372</td>\n",
              "      <td>-92.541060</td>\n",
              "      <td>-85.111657</td>\n",
              "      <td>1.257385</td>\n",
              "      <td>8663.109154</td>\n",
              "      <td>0.876529</td>\n",
              "      <td>15887.754247</td>\n",
              "      <td>0.936231</td>\n",
              "      <td>126.046635</td>\n",
              "      <td>-62.197363</td>\n",
              "      <td>-1.147830</td>\n",
              "      <td>-0.015316</td>\n",
              "      <td>0.877519</td>\n",
              "      <td>-65.765800</td>\n",
              "      <td>-0.824460</td>\n",
              "      <td>-0.070981</td>\n",
              "      <td>4.066897</td>\n",
              "      <td>-47.238067</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>13125000 rows × 21 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "             acc_x     acc_y     acc_z  ...  gy_pitch  gy_math_roll  gy_math_pitch\n",
              "0         1.206087 -0.179371 -0.148447  ... -0.013443      0.770209     -43.957305\n",
              "1         1.287696 -0.198974 -0.182444  ...  0.006532     -0.374246     -57.505519\n",
              "2         1.304609 -0.195114 -0.253382  ... -0.071194      4.079137     -60.189701\n",
              "3         1.293095 -0.230366 -0.215210  ...  0.045021     -2.579501     -62.764338\n",
              "4         1.300887 -0.187757 -0.222523  ...  0.066565     -3.813876     -63.964038\n",
              "...            ...       ...       ...  ...       ...           ...            ...\n",
              "13124995 -0.028493 -0.994816  0.364849  ... -0.052743      3.021958     -48.634155\n",
              "13124996 -0.031233 -1.023963  0.342354  ... -0.059134      3.388101     -48.104839\n",
              "13124997 -0.044125 -0.953804  0.386487  ... -0.061625      3.530877     -47.684732\n",
              "13124998 -0.041707 -0.868214  0.404388  ... -0.067849      3.887440     -47.392098\n",
              "13124999 -0.014338 -0.853726  0.384025  ... -0.070981      4.066897     -47.238067\n",
              "\n",
              "[13125000 rows x 21 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KG0UxyQzqy38",
        "outputId": "912cd85a-bdd3-4066-b4aa-3b0a5c82f767"
      },
      "source": [
        "# Scaling 원하는 걸로 사용\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import Normalizer\n",
        "\n",
        "scaler = RobustScaler()\n",
        "train = scaler.fit_transform(train)\n",
        "test.drop(['id', 'time'], axis=1, inplace=True)\n",
        "test = scaler.transform(test)\n",
        "train"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1.93489817,  0.03657354, -0.31330862, ...,  0.05924213,\n",
              "        -0.05924213, -0.71658616],\n",
              "       [ 2.01969699,  0.00673528, -0.37289479, ...,  0.07768724,\n",
              "        -0.07768724, -0.94204715],\n",
              "       [ 2.0372705 ,  0.01260973, -0.49722803, ...,  0.0059123 ,\n",
              "        -0.0059123 , -0.98671563],\n",
              "       ...,\n",
              "       [ 0.63583413, -1.14225409,  0.6242618 , ...,  0.01474858,\n",
              "        -0.01474858, -0.77861568],\n",
              "       [ 0.63834688, -1.01197012,  0.65563607, ...,  0.00900187,\n",
              "        -0.00900187, -0.77374585],\n",
              "       [ 0.66678483, -0.98991733,  0.61994599, ...,  0.00610958,\n",
              "        -0.00610958, -0.77118257]])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqGdCWomjITD"
      },
      "source": [
        "import tensorflow as tf \n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dropout, LSTM, Input\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.layers import Activation, GlobalAveragePooling1D\n",
        "from keras.layers import Dense, Flatten, BatchNormalization\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.models import Model\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras import regularizers\n",
        "from keras.models import load_model\n",
        "from keras.layers.merge import concatenate"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3G_9MwV3lOCr",
        "outputId": "996f10db-4dda-458d-8c4f-540c4bd3d851"
      },
      "source": [
        "len_features = train.shape[1] # feature 갯수\n",
        "X = train.reshape(-1, 600, len_features)\n",
        "X.shape"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(21875, 600, 21)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L9BwD5mvnD21",
        "outputId": "e19d614a-e1a7-4751-8923-c62a89473f18"
      },
      "source": [
        "y = to_categorical(y_train_total) \n",
        "y.shape"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(21875, 61)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Msg3BbJi_9Tj",
        "outputId": "a2af3107-6a45-41f5-f49d-6812ba2ba119"
      },
      "source": [
        "y_train_total.shape"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(21875,)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yd01SsFRoSc2"
      },
      "source": [
        "epochs, batch_size = 100, 64 # Ram 24GB 기준 256 이상 reset될 가능성 높음"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RMA0WAJ6q6_2",
        "outputId": "1e5a5a30-a7c4-40a3-d91c-e212bf75b802"
      },
      "source": [
        "test_X = test.reshape(-1, 600, len_features)\n",
        "test_X.shape"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(782, 600, 21)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWxrKNusqy3-"
      },
      "source": [
        "epochs, batch_size = 30, 64\n",
        "n_features, n_outputs = X.shape[2], y.shape[1]\n",
        "# reshape data into time steps of sub-sequences\n",
        "n_steps, n_length = 6, 100\n",
        "X = X.reshape((X.shape[0], n_steps, n_length, n_features))\n",
        "test_X = test_X.reshape((test_X.shape[0], n_steps, n_length, n_features))"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsJbpKCIjBoc"
      },
      "source": [
        "class Models:\n",
        "    # 각종 사이즈는 다양하게 적용하여 stacking 쌓아 올리기\n",
        "    def define_model_0():\n",
        "        model = Sequential()\n",
        "        model.add(TimeDistributed(Conv1D(filters=64, kernel_size=3, activation='relu'), input_shape=(None,n_length,n_features)))\n",
        "        model.add(TimeDistributed(Conv1D(filters=64, kernel_size=3, activation='relu')))\n",
        "        model.add(TimeDistributed(Dropout(0.5)))\n",
        "        model.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n",
        "        model.add(TimeDistributed(Flatten()))\n",
        "        model.add(LSTM(32))\n",
        "        model.add(Dropout(0.5))\n",
        "        model.add(Dense(128, activation='relu'))\n",
        "        model.add(Dense(n_outputs, activation='softmax'))\n",
        "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "        return model\n",
        "\n",
        "    def define_model_1():\n",
        "        model = Sequential()\n",
        "        model.add(TimeDistributed(Conv1D(filters=32, kernel_size=3, activation='relu'), input_shape=(None,n_length,n_features)))\n",
        "        model.add(TimeDistributed(Dropout(0.5)))\n",
        "        model.add(TimeDistributed(Conv1D(filters=32, kernel_size=3, activation='relu')))\n",
        "        model.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n",
        "        model.add(TimeDistributed(Flatten()))\n",
        "        model.add(LSTM(32))\n",
        "        model.add(Dropout(0.5))\n",
        "        model.add(Dense(128, activation='relu'))\n",
        "        model.add(Dense(n_outputs, activation='softmax'))\n",
        "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "        return model\n",
        "\n",
        "    def define_model_2():\n",
        "        model = Sequential()\n",
        "        model.add(TimeDistributed(Conv1D(filters=64, kernel_size=6, activation='relu'), input_shape=(None,n_length,n_features)))\n",
        "        model.add(TimeDistributed(Conv1D(filters=64, kernel_size=3, activation='relu')))\n",
        "        model.add(TimeDistributed(Dropout(0.5)))\n",
        "        model.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n",
        "        model.add(TimeDistributed(Flatten()))\n",
        "        model.add(LSTM(32))\n",
        "        model.add(Dropout(0.5))\n",
        "        model.add(Dense(128, activation='relu'))\n",
        "        model.add(Dense(n_outputs, activation='softmax'))\n",
        "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "        return model\n",
        "\n",
        "    def define_model_3():\n",
        "        model = Sequential()\n",
        "        model.add(TimeDistributed(Conv1D(filters=32, kernel_size=3, activation='relu'), input_shape=(None,n_length,n_features)))\n",
        "        model.add(TimeDistributed(Conv1D(filters=64, kernel_size=6, activation='relu')))\n",
        "        model.add(TimeDistributed(Dropout(0.5)))\n",
        "        model.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n",
        "        model.add(TimeDistributed(Flatten()))\n",
        "        model.add(LSTM(32))\n",
        "        model.add(Dropout(0.3))\n",
        "        model.add(Dense(128, activation='relu'))\n",
        "        model.add(Dense(n_outputs, activation='softmax'))\n",
        "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "        return model\n",
        "\n",
        "    def define_model_4():\n",
        "        model = Sequential()\n",
        "        model.add(TimeDistributed(Conv1D(filters=32, kernel_size=3, activation='relu'), input_shape=(None,n_length,n_features)))\n",
        "        model.add(TimeDistributed(Conv1D(filters=32, kernel_size=3, activation='relu')))\n",
        "        model.add(TimeDistributed(Dropout(0.5)))\n",
        "        model.add(TimeDistributed(GlobalAveragePooling1D()))\n",
        "        model.add(TimeDistributed(Flatten()))\n",
        "        model.add(LSTM(32))\n",
        "        model.add(Dropout(0.5))\n",
        "        model.add(Dense(128, activation='relu'))\n",
        "        model.add(Dense(n_outputs, activation='softmax'))\n",
        "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "        return model\n",
        "\n",
        "    def define_model_5():\n",
        "        model = Sequential()\n",
        "        model.add(TimeDistributed(Conv1D(filters=32, kernel_size=3, activation='relu'), input_shape=(None,n_length,n_features)))\n",
        "        model.add(TimeDistributed(BatchNormalization()))\n",
        "        model.add(TimeDistributed(Conv1D(filters=64, kernel_size=6, activation='relu')))\n",
        "        model.add(TimeDistributed(Dropout(0.5)))\n",
        "        model.add(TimeDistributed(GlobalAveragePooling1D()))\n",
        "        model.add(LSTM(32))\n",
        "        model.add(Dropout(0.5))\n",
        "        model.add(Dense(128, activation='relu'))\n",
        "        model.add(Dense(n_outputs, activation='softmax'))\n",
        "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "        return model\n",
        "\n",
        "    def define_model_6():\n",
        "        model = Sequential()\n",
        "        model.add(TimeDistributed(Conv1D(filters=64, kernel_size=3, activation='relu'), input_shape=(None,n_length,n_features)))\n",
        "        model.add(TimeDistributed(BatchNormalization()))\n",
        "        model.add(TimeDistributed(Conv1D(filters=64, kernel_size=3, activation='relu')))\n",
        "        model.add(TimeDistributed(Dropout(0.5)))\n",
        "        model.add(TimeDistributed(GlobalAveragePooling1D()))\n",
        "        model.add(LSTM(32))\n",
        "        model.add(Dropout(0.3))\n",
        "        model.add(Dense(128, activation='relu'))\n",
        "        model.add(Dense(n_outputs, activation='softmax'))\n",
        "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "        return model\n",
        "\n",
        "    def define_model_7():\n",
        "        model = Sequential()\n",
        "        model.add(TimeDistributed(Conv1D(filters=64, kernel_size=6, activation='relu'), input_shape=(None,n_length,n_features)))\n",
        "        model.add(TimeDistributed(BatchNormalization()))\n",
        "        model.add(TimeDistributed(Conv1D(filters=64, kernel_size=3, activation='relu')))\n",
        "        model.add(TimeDistributed(Dropout(0.5)))\n",
        "        model.add(TimeDistributed(GlobalAveragePooling1D()))\n",
        "        model.add(LSTM(32))\n",
        "        model.add(Dropout(0.5))\n",
        "        model.add(Dense(128, activation='relu'))\n",
        "        model.add(Dense(n_outputs, activation='softmax'))\n",
        "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "        return model"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvJ-AalijBrV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff859ac5-2b2f-41cb-b715-737e3ab30650"
      },
      "source": [
        "# 위의 모델들 학습하면서 stacking\n",
        "for i in range(8):\n",
        "    model = getattr(Models, f'define_model_{i}')()\n",
        "    checkpoint_path = \"checkpoint/cp.ckpt\"\n",
        "    cp_callback = ModelCheckpoint(filepath=checkpoint_path, monitor='loss', \n",
        "                                verbose=1, save_weights_only=True, \n",
        "                                save_best_only=True, mode='min')\n",
        "    early_stopping = EarlyStopping(monitor='loss', patience=30, mode='min')\n",
        "    model.fit(X, y, epochs=epochs, batch_size=batch_size, \n",
        "            validation_split=0.2, callbacks=[early_stopping, cp_callback])\n",
        "    model.save(f'models/model_{i}.h5')\n",
        "    tf.keras.backend.clear_session()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "274/274 [==============================] - 39s 24ms/step - loss: 2.7642 - accuracy: 0.4816 - val_loss: 2.5162 - val_accuracy: 0.4862\n",
            "\n",
            "Epoch 00001: loss improved from inf to 2.76416, saving model to checkpoint/cp.ckpt\n",
            "Epoch 2/30\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 2.3773 - accuracy: 0.4922 - val_loss: 2.1374 - val_accuracy: 0.5138\n",
            "\n",
            "Epoch 00002: loss improved from 2.76416 to 2.37725, saving model to checkpoint/cp.ckpt\n",
            "Epoch 3/30\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 2.1975 - accuracy: 0.5034 - val_loss: 2.0006 - val_accuracy: 0.5351\n",
            "\n",
            "Epoch 00003: loss improved from 2.37725 to 2.19747, saving model to checkpoint/cp.ckpt\n",
            "Epoch 4/30\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 2.1003 - accuracy: 0.5127 - val_loss: 1.9267 - val_accuracy: 0.5365\n",
            "\n",
            "Epoch 00004: loss improved from 2.19747 to 2.10028, saving model to checkpoint/cp.ckpt\n",
            "Epoch 5/30\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 2.0237 - accuracy: 0.5193 - val_loss: 1.8143 - val_accuracy: 0.5541\n",
            "\n",
            "Epoch 00005: loss improved from 2.10028 to 2.02367, saving model to checkpoint/cp.ckpt\n",
            "Epoch 6/30\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 1.9386 - accuracy: 0.5333 - val_loss: 1.7809 - val_accuracy: 0.5621\n",
            "\n",
            "Epoch 00006: loss improved from 2.02367 to 1.93864, saving model to checkpoint/cp.ckpt\n",
            "Epoch 7/30\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 1.8796 - accuracy: 0.5401 - val_loss: 1.6759 - val_accuracy: 0.5744\n",
            "\n",
            "Epoch 00007: loss improved from 1.93864 to 1.87960, saving model to checkpoint/cp.ckpt\n",
            "Epoch 8/30\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 1.8204 - accuracy: 0.5469 - val_loss: 1.6284 - val_accuracy: 0.5810\n",
            "\n",
            "Epoch 00008: loss improved from 1.87960 to 1.82043, saving model to checkpoint/cp.ckpt\n",
            "Epoch 9/30\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 1.7710 - accuracy: 0.5561 - val_loss: 1.5877 - val_accuracy: 0.5963\n",
            "\n",
            "Epoch 00009: loss improved from 1.82043 to 1.77097, saving model to checkpoint/cp.ckpt\n",
            "Epoch 10/30\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 1.7468 - accuracy: 0.5602 - val_loss: 1.5918 - val_accuracy: 0.6053\n",
            "\n",
            "Epoch 00010: loss improved from 1.77097 to 1.74683, saving model to checkpoint/cp.ckpt\n",
            "Epoch 11/30\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 1.7055 - accuracy: 0.5676 - val_loss: 1.5068 - val_accuracy: 0.6073\n",
            "\n",
            "Epoch 00011: loss improved from 1.74683 to 1.70549, saving model to checkpoint/cp.ckpt\n",
            "Epoch 12/30\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 1.6696 - accuracy: 0.5748 - val_loss: 1.4700 - val_accuracy: 0.6174\n",
            "\n",
            "Epoch 00012: loss improved from 1.70549 to 1.66956, saving model to checkpoint/cp.ckpt\n",
            "Epoch 13/30\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 1.6382 - accuracy: 0.5794 - val_loss: 1.4522 - val_accuracy: 0.6249\n",
            "\n",
            "Epoch 00013: loss improved from 1.66956 to 1.63816, saving model to checkpoint/cp.ckpt\n",
            "Epoch 14/30\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 1.5984 - accuracy: 0.5887 - val_loss: 1.4408 - val_accuracy: 0.6213\n",
            "\n",
            "Epoch 00014: loss improved from 1.63816 to 1.59839, saving model to checkpoint/cp.ckpt\n",
            "Epoch 15/30\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 1.5707 - accuracy: 0.5942 - val_loss: 1.4361 - val_accuracy: 0.6240\n",
            "\n",
            "Epoch 00015: loss improved from 1.59839 to 1.57072, saving model to checkpoint/cp.ckpt\n",
            "Epoch 16/30\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 1.5530 - accuracy: 0.5993 - val_loss: 1.4188 - val_accuracy: 0.6249\n",
            "\n",
            "Epoch 00016: loss improved from 1.57072 to 1.55302, saving model to checkpoint/cp.ckpt\n",
            "Epoch 17/30\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 1.5212 - accuracy: 0.6027 - val_loss: 1.3737 - val_accuracy: 0.6343\n",
            "\n",
            "Epoch 00017: loss improved from 1.55302 to 1.52116, saving model to checkpoint/cp.ckpt\n",
            "Epoch 18/30\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 1.5081 - accuracy: 0.6114 - val_loss: 1.3644 - val_accuracy: 0.6309\n",
            "\n",
            "Epoch 00018: loss improved from 1.52116 to 1.50806, saving model to checkpoint/cp.ckpt\n",
            "Epoch 19/30\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 1.4849 - accuracy: 0.6145 - val_loss: 1.3397 - val_accuracy: 0.6523\n",
            "\n",
            "Epoch 00019: loss improved from 1.50806 to 1.48491, saving model to checkpoint/cp.ckpt\n",
            "Epoch 20/30\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 1.4655 - accuracy: 0.6170 - val_loss: 1.3511 - val_accuracy: 0.6448\n",
            "\n",
            "Epoch 00020: loss improved from 1.48491 to 1.46553, saving model to checkpoint/cp.ckpt\n",
            "Epoch 21/30\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 1.4341 - accuracy: 0.6234 - val_loss: 1.3583 - val_accuracy: 0.6457\n",
            "\n",
            "Epoch 00021: loss improved from 1.46553 to 1.43412, saving model to checkpoint/cp.ckpt\n",
            "Epoch 22/30\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 1.4230 - accuracy: 0.6259 - val_loss: 1.2755 - val_accuracy: 0.6631\n",
            "\n",
            "Epoch 00022: loss improved from 1.43412 to 1.42296, saving model to checkpoint/cp.ckpt\n",
            "Epoch 23/30\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 1.3991 - accuracy: 0.6305 - val_loss: 1.3020 - val_accuracy: 0.6578\n",
            "\n",
            "Epoch 00023: loss improved from 1.42296 to 1.39907, saving model to checkpoint/cp.ckpt\n",
            "Epoch 24/30\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 1.3866 - accuracy: 0.6298 - val_loss: 1.3100 - val_accuracy: 0.6546\n",
            "\n",
            "Epoch 00024: loss improved from 1.39907 to 1.38663, saving model to checkpoint/cp.ckpt\n",
            "Epoch 25/30\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 1.3589 - accuracy: 0.6398 - val_loss: 1.2832 - val_accuracy: 0.6619\n",
            "\n",
            "Epoch 00025: loss improved from 1.38663 to 1.35892, saving model to checkpoint/cp.ckpt\n",
            "Epoch 26/30\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 1.3586 - accuracy: 0.6405 - val_loss: 1.2671 - val_accuracy: 0.6601\n",
            "\n",
            "Epoch 00026: loss improved from 1.35892 to 1.35859, saving model to checkpoint/cp.ckpt\n",
            "Epoch 27/30\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 1.3303 - accuracy: 0.6430 - val_loss: 1.2810 - val_accuracy: 0.6626\n",
            "\n",
            "Epoch 00027: loss improved from 1.35859 to 1.33027, saving model to checkpoint/cp.ckpt\n",
            "Epoch 28/30\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 1.3188 - accuracy: 0.6462 - val_loss: 1.2723 - val_accuracy: 0.6661\n",
            "\n",
            "Epoch 00028: loss improved from 1.33027 to 1.31880, saving model to checkpoint/cp.ckpt\n",
            "Epoch 29/30\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 1.3003 - accuracy: 0.6529 - val_loss: 1.2433 - val_accuracy: 0.6718\n",
            "\n",
            "Epoch 00029: loss improved from 1.31880 to 1.30030, saving model to checkpoint/cp.ckpt\n",
            "Epoch 30/30\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 1.2807 - accuracy: 0.6527 - val_loss: 1.2551 - val_accuracy: 0.6635\n",
            "\n",
            "Epoch 00030: loss improved from 1.30030 to 1.28069, saving model to checkpoint/cp.ckpt\n",
            "Epoch 1/30\n",
            "274/274 [==============================] - 8s 18ms/step - loss: 2.8013 - accuracy: 0.4803 - val_loss: 2.4888 - val_accuracy: 0.5038\n",
            "\n",
            "Epoch 00001: loss improved from inf to 2.80135, saving model to checkpoint/cp.ckpt\n",
            "Epoch 2/30\n",
            "274/274 [==============================] - 4s 13ms/step - loss: 2.3688 - accuracy: 0.4992 - val_loss: 2.2407 - val_accuracy: 0.5127\n",
            "\n",
            "Epoch 00002: loss improved from 2.80135 to 2.36877, saving model to checkpoint/cp.ckpt\n",
            "Epoch 3/30\n",
            "274/274 [==============================] - 4s 13ms/step - loss: 2.1851 - accuracy: 0.5090 - val_loss: 2.0962 - val_accuracy: 0.5296\n",
            "\n",
            "Epoch 00003: loss improved from 2.36877 to 2.18511, saving model to checkpoint/cp.ckpt\n",
            "Epoch 4/30\n",
            "274/274 [==============================] - 4s 13ms/step - loss: 2.0603 - accuracy: 0.5221 - val_loss: 1.9118 - val_accuracy: 0.5504\n",
            "\n",
            "Epoch 00004: loss improved from 2.18511 to 2.06029, saving model to checkpoint/cp.ckpt\n",
            "Epoch 5/30\n",
            "274/274 [==============================] - 4s 13ms/step - loss: 1.9505 - accuracy: 0.5350 - val_loss: 1.8121 - val_accuracy: 0.5557\n",
            "\n",
            "Epoch 00005: loss improved from 2.06029 to 1.95049, saving model to checkpoint/cp.ckpt\n",
            "Epoch 6/30\n",
            "274/274 [==============================] - 4s 13ms/step - loss: 1.8719 - accuracy: 0.5447 - val_loss: 1.7702 - val_accuracy: 0.5573\n",
            "\n",
            "Epoch 00006: loss improved from 1.95049 to 1.87189, saving model to checkpoint/cp.ckpt\n",
            "Epoch 7/30\n",
            "274/274 [==============================] - 4s 13ms/step - loss: 1.8052 - accuracy: 0.5529 - val_loss: 1.7543 - val_accuracy: 0.5607\n",
            "\n",
            "Epoch 00007: loss improved from 1.87189 to 1.80517, saving model to checkpoint/cp.ckpt\n",
            "Epoch 8/30\n",
            "274/274 [==============================] - 4s 13ms/step - loss: 1.7643 - accuracy: 0.5598 - val_loss: 1.6518 - val_accuracy: 0.5762\n",
            "\n",
            "Epoch 00008: loss improved from 1.80517 to 1.76425, saving model to checkpoint/cp.ckpt\n",
            "Epoch 9/30\n",
            "274/274 [==============================] - 4s 13ms/step - loss: 1.7034 - accuracy: 0.5677 - val_loss: 1.6101 - val_accuracy: 0.5760\n",
            "\n",
            "Epoch 00009: loss improved from 1.76425 to 1.70340, saving model to checkpoint/cp.ckpt\n",
            "Epoch 10/30\n",
            "274/274 [==============================] - 4s 13ms/step - loss: 1.6560 - accuracy: 0.5738 - val_loss: 1.6195 - val_accuracy: 0.5760\n",
            "\n",
            "Epoch 00010: loss improved from 1.70340 to 1.65598, saving model to checkpoint/cp.ckpt\n",
            "Epoch 11/30\n",
            "274/274 [==============================] - 4s 13ms/step - loss: 1.6267 - accuracy: 0.5801 - val_loss: 1.5011 - val_accuracy: 0.5909\n",
            "\n",
            "Epoch 00011: loss improved from 1.65598 to 1.62673, saving model to checkpoint/cp.ckpt\n",
            "Epoch 12/30\n",
            "274/274 [==============================] - 4s 13ms/step - loss: 1.5909 - accuracy: 0.5878 - val_loss: 1.5382 - val_accuracy: 0.5845\n",
            "\n",
            "Epoch 00012: loss improved from 1.62673 to 1.59086, saving model to checkpoint/cp.ckpt\n",
            "Epoch 13/30\n",
            "274/274 [==============================] - 4s 13ms/step - loss: 1.5602 - accuracy: 0.5967 - val_loss: 1.4767 - val_accuracy: 0.6091\n",
            "\n",
            "Epoch 00013: loss improved from 1.59086 to 1.56023, saving model to checkpoint/cp.ckpt\n",
            "Epoch 14/30\n",
            "274/274 [==============================] - 4s 13ms/step - loss: 1.5197 - accuracy: 0.6005 - val_loss: 1.4312 - val_accuracy: 0.6135\n",
            "\n",
            "Epoch 00014: loss improved from 1.56023 to 1.51969, saving model to checkpoint/cp.ckpt\n",
            "Epoch 15/30\n",
            "274/274 [==============================] - 4s 13ms/step - loss: 1.4948 - accuracy: 0.6063 - val_loss: 1.4273 - val_accuracy: 0.6151\n",
            "\n",
            "Epoch 00015: loss improved from 1.51969 to 1.49483, saving model to checkpoint/cp.ckpt\n",
            "Epoch 16/30\n",
            "274/274 [==============================] - 4s 13ms/step - loss: 1.4646 - accuracy: 0.6156 - val_loss: 1.4058 - val_accuracy: 0.6110\n",
            "\n",
            "Epoch 00016: loss improved from 1.49483 to 1.46463, saving model to checkpoint/cp.ckpt\n",
            "Epoch 17/30\n",
            "274/274 [==============================] - 4s 14ms/step - loss: 1.4335 - accuracy: 0.6211 - val_loss: 1.3923 - val_accuracy: 0.6251\n",
            "\n",
            "Epoch 00017: loss improved from 1.46463 to 1.43346, saving model to checkpoint/cp.ckpt\n",
            "Epoch 18/30\n",
            "274/274 [==============================] - 4s 13ms/step - loss: 1.4115 - accuracy: 0.6262 - val_loss: 1.3778 - val_accuracy: 0.6283\n",
            "\n",
            "Epoch 00018: loss improved from 1.43346 to 1.41152, saving model to checkpoint/cp.ckpt\n",
            "Epoch 19/30\n",
            "274/274 [==============================] - 4s 13ms/step - loss: 1.3856 - accuracy: 0.6306 - val_loss: 1.3661 - val_accuracy: 0.6338\n",
            "\n",
            "Epoch 00019: loss improved from 1.41152 to 1.38562, saving model to checkpoint/cp.ckpt\n",
            "Epoch 20/30\n",
            "274/274 [==============================] - 4s 13ms/step - loss: 1.3784 - accuracy: 0.6337 - val_loss: 1.3435 - val_accuracy: 0.6304\n",
            "\n",
            "Epoch 00020: loss improved from 1.38562 to 1.37835, saving model to checkpoint/cp.ckpt\n",
            "Epoch 21/30\n",
            "274/274 [==============================] - 3s 13ms/step - loss: 1.3446 - accuracy: 0.6389 - val_loss: 1.3218 - val_accuracy: 0.6402\n",
            "\n",
            "Epoch 00021: loss improved from 1.37835 to 1.34464, saving model to checkpoint/cp.ckpt\n",
            "Epoch 22/30\n",
            "274/274 [==============================] - 4s 13ms/step - loss: 1.3231 - accuracy: 0.6440 - val_loss: 1.2938 - val_accuracy: 0.6379\n",
            "\n",
            "Epoch 00022: loss improved from 1.34464 to 1.32311, saving model to checkpoint/cp.ckpt\n",
            "Epoch 23/30\n",
            "274/274 [==============================] - 4s 13ms/step - loss: 1.2983 - accuracy: 0.6501 - val_loss: 1.2838 - val_accuracy: 0.6473\n",
            "\n",
            "Epoch 00023: loss improved from 1.32311 to 1.29833, saving model to checkpoint/cp.ckpt\n",
            "Epoch 24/30\n",
            "274/274 [==============================] - 4s 13ms/step - loss: 1.2824 - accuracy: 0.6538 - val_loss: 1.2699 - val_accuracy: 0.6587\n",
            "\n",
            "Epoch 00024: loss improved from 1.29833 to 1.28243, saving model to checkpoint/cp.ckpt\n",
            "Epoch 25/30\n",
            "274/274 [==============================] - 4s 13ms/step - loss: 1.2650 - accuracy: 0.6580 - val_loss: 1.2753 - val_accuracy: 0.6619\n",
            "\n",
            "Epoch 00025: loss improved from 1.28243 to 1.26495, saving model to checkpoint/cp.ckpt\n",
            "Epoch 26/30\n",
            "274/274 [==============================] - 4s 13ms/step - loss: 1.2446 - accuracy: 0.6663 - val_loss: 1.2817 - val_accuracy: 0.6530\n",
            "\n",
            "Epoch 00026: loss improved from 1.26495 to 1.24457, saving model to checkpoint/cp.ckpt\n",
            "Epoch 27/30\n",
            "274/274 [==============================] - 4s 13ms/step - loss: 1.2391 - accuracy: 0.6623 - val_loss: 1.2472 - val_accuracy: 0.6622\n",
            "\n",
            "Epoch 00027: loss improved from 1.24457 to 1.23913, saving model to checkpoint/cp.ckpt\n",
            "Epoch 28/30\n",
            "274/274 [==============================] - 4s 14ms/step - loss: 1.2152 - accuracy: 0.6645 - val_loss: 1.2556 - val_accuracy: 0.6560\n",
            "\n",
            "Epoch 00028: loss improved from 1.23913 to 1.21518, saving model to checkpoint/cp.ckpt\n",
            "Epoch 29/30\n",
            "274/274 [==============================] - 4s 13ms/step - loss: 1.2104 - accuracy: 0.6706 - val_loss: 1.2616 - val_accuracy: 0.6622\n",
            "\n",
            "Epoch 00029: loss improved from 1.21518 to 1.21043, saving model to checkpoint/cp.ckpt\n",
            "Epoch 30/30\n",
            "274/274 [==============================] - 4s 13ms/step - loss: 1.1977 - accuracy: 0.6728 - val_loss: 1.2334 - val_accuracy: 0.6677\n",
            "\n",
            "Epoch 00030: loss improved from 1.21043 to 1.19767, saving model to checkpoint/cp.ckpt\n",
            "Epoch 1/30\n",
            "274/274 [==============================] - 10s 24ms/step - loss: 2.7345 - accuracy: 0.4791 - val_loss: 2.4061 - val_accuracy: 0.5035\n",
            "\n",
            "Epoch 00001: loss improved from inf to 2.73452, saving model to checkpoint/cp.ckpt\n",
            "Epoch 2/30\n",
            "274/274 [==============================] - 5s 19ms/step - loss: 2.2916 - accuracy: 0.5007 - val_loss: 2.1250 - val_accuracy: 0.5170\n",
            "\n",
            "Epoch 00002: loss improved from 2.73452 to 2.29164, saving model to checkpoint/cp.ckpt\n",
            "Epoch 3/30\n",
            "274/274 [==============================] - 5s 19ms/step - loss: 2.1388 - accuracy: 0.5094 - val_loss: 1.9594 - val_accuracy: 0.5378\n",
            "\n",
            "Epoch 00003: loss improved from 2.29164 to 2.13877, saving model to checkpoint/cp.ckpt\n",
            "Epoch 4/30\n",
            "274/274 [==============================] - 5s 19ms/step - loss: 2.0368 - accuracy: 0.5227 - val_loss: 1.8562 - val_accuracy: 0.5445\n",
            "\n",
            "Epoch 00004: loss improved from 2.13877 to 2.03685, saving model to checkpoint/cp.ckpt\n",
            "Epoch 5/30\n",
            "274/274 [==============================] - 5s 19ms/step - loss: 1.9671 - accuracy: 0.5287 - val_loss: 1.7748 - val_accuracy: 0.5547\n",
            "\n",
            "Epoch 00005: loss improved from 2.03685 to 1.96713, saving model to checkpoint/cp.ckpt\n",
            "Epoch 6/30\n",
            "274/274 [==============================] - 5s 19ms/step - loss: 1.9075 - accuracy: 0.5372 - val_loss: 1.6998 - val_accuracy: 0.5623\n",
            "\n",
            "Epoch 00006: loss improved from 1.96713 to 1.90750, saving model to checkpoint/cp.ckpt\n",
            "Epoch 7/30\n",
            "274/274 [==============================] - 5s 19ms/step - loss: 1.8515 - accuracy: 0.5458 - val_loss: 1.6248 - val_accuracy: 0.5767\n",
            "\n",
            "Epoch 00007: loss improved from 1.90750 to 1.85148, saving model to checkpoint/cp.ckpt\n",
            "Epoch 8/30\n",
            "274/274 [==============================] - 5s 19ms/step - loss: 1.7954 - accuracy: 0.5546 - val_loss: 1.5878 - val_accuracy: 0.5813\n",
            "\n",
            "Epoch 00008: loss improved from 1.85148 to 1.79539, saving model to checkpoint/cp.ckpt\n",
            "Epoch 9/30\n",
            "274/274 [==============================] - 5s 19ms/step - loss: 1.7468 - accuracy: 0.5620 - val_loss: 1.5204 - val_accuracy: 0.6069\n",
            "\n",
            "Epoch 00009: loss improved from 1.79539 to 1.74683, saving model to checkpoint/cp.ckpt\n",
            "Epoch 10/30\n",
            "274/274 [==============================] - 5s 19ms/step - loss: 1.6924 - accuracy: 0.5731 - val_loss: 1.4960 - val_accuracy: 0.5998\n",
            "\n",
            "Epoch 00010: loss improved from 1.74683 to 1.69236, saving model to checkpoint/cp.ckpt\n",
            "Epoch 11/30\n",
            "274/274 [==============================] - 5s 19ms/step - loss: 1.6607 - accuracy: 0.5779 - val_loss: 1.4608 - val_accuracy: 0.6139\n",
            "\n",
            "Epoch 00011: loss improved from 1.69236 to 1.66067, saving model to checkpoint/cp.ckpt\n",
            "Epoch 12/30\n",
            "274/274 [==============================] - 5s 19ms/step - loss: 1.6267 - accuracy: 0.5857 - val_loss: 1.4257 - val_accuracy: 0.6231\n",
            "\n",
            "Epoch 00012: loss improved from 1.66067 to 1.62672, saving model to checkpoint/cp.ckpt\n",
            "Epoch 13/30\n",
            "274/274 [==============================] - 5s 19ms/step - loss: 1.5887 - accuracy: 0.5961 - val_loss: 1.3922 - val_accuracy: 0.6315\n",
            "\n",
            "Epoch 00013: loss improved from 1.62672 to 1.58872, saving model to checkpoint/cp.ckpt\n",
            "Epoch 14/30\n",
            "274/274 [==============================] - 5s 19ms/step - loss: 1.5415 - accuracy: 0.6041 - val_loss: 1.3721 - val_accuracy: 0.6363\n",
            "\n",
            "Epoch 00014: loss improved from 1.58872 to 1.54151, saving model to checkpoint/cp.ckpt\n",
            "Epoch 15/30\n",
            "274/274 [==============================] - 5s 19ms/step - loss: 1.5180 - accuracy: 0.6135 - val_loss: 1.3500 - val_accuracy: 0.6334\n",
            "\n",
            "Epoch 00015: loss improved from 1.54151 to 1.51802, saving model to checkpoint/cp.ckpt\n",
            "Epoch 16/30\n",
            "274/274 [==============================] - 5s 19ms/step - loss: 1.4768 - accuracy: 0.6169 - val_loss: 1.3145 - val_accuracy: 0.6443\n",
            "\n",
            "Epoch 00016: loss improved from 1.51802 to 1.47676, saving model to checkpoint/cp.ckpt\n",
            "Epoch 17/30\n",
            "274/274 [==============================] - 5s 19ms/step - loss: 1.4455 - accuracy: 0.6208 - val_loss: 1.3130 - val_accuracy: 0.6398\n",
            "\n",
            "Epoch 00017: loss improved from 1.47676 to 1.44553, saving model to checkpoint/cp.ckpt\n",
            "Epoch 18/30\n",
            "274/274 [==============================] - 5s 19ms/step - loss: 1.4238 - accuracy: 0.6293 - val_loss: 1.3133 - val_accuracy: 0.6473\n",
            "\n",
            "Epoch 00018: loss improved from 1.44553 to 1.42383, saving model to checkpoint/cp.ckpt\n",
            "Epoch 19/30\n",
            "274/274 [==============================] - 5s 19ms/step - loss: 1.4034 - accuracy: 0.6313 - val_loss: 1.2616 - val_accuracy: 0.6571\n",
            "\n",
            "Epoch 00019: loss improved from 1.42383 to 1.40340, saving model to checkpoint/cp.ckpt\n",
            "Epoch 20/30\n",
            "274/274 [==============================] - 5s 19ms/step - loss: 1.3952 - accuracy: 0.6322 - val_loss: 1.2581 - val_accuracy: 0.6670\n",
            "\n",
            "Epoch 00020: loss improved from 1.40340 to 1.39525, saving model to checkpoint/cp.ckpt\n",
            "Epoch 21/30\n",
            "274/274 [==============================] - 5s 19ms/step - loss: 1.3591 - accuracy: 0.6419 - val_loss: 1.2298 - val_accuracy: 0.6640\n",
            "\n",
            "Epoch 00021: loss improved from 1.39525 to 1.35910, saving model to checkpoint/cp.ckpt\n",
            "Epoch 22/30\n",
            "274/274 [==============================] - 5s 19ms/step - loss: 1.3365 - accuracy: 0.6463 - val_loss: 1.2160 - val_accuracy: 0.6757\n",
            "\n",
            "Epoch 00022: loss improved from 1.35910 to 1.33651, saving model to checkpoint/cp.ckpt\n",
            "Epoch 23/30\n",
            "274/274 [==============================] - 5s 19ms/step - loss: 1.3163 - accuracy: 0.6510 - val_loss: 1.2497 - val_accuracy: 0.6615\n",
            "\n",
            "Epoch 00023: loss improved from 1.33651 to 1.31627, saving model to checkpoint/cp.ckpt\n",
            "Epoch 24/30\n",
            "274/274 [==============================] - 5s 19ms/step - loss: 1.2927 - accuracy: 0.6546 - val_loss: 1.2395 - val_accuracy: 0.6658\n",
            "\n",
            "Epoch 00024: loss improved from 1.31627 to 1.29268, saving model to checkpoint/cp.ckpt\n",
            "Epoch 25/30\n",
            "274/274 [==============================] - 5s 19ms/step - loss: 1.2695 - accuracy: 0.6651 - val_loss: 1.1957 - val_accuracy: 0.6825\n",
            "\n",
            "Epoch 00025: loss improved from 1.29268 to 1.26954, saving model to checkpoint/cp.ckpt\n",
            "Epoch 26/30\n",
            "274/274 [==============================] - 5s 19ms/step - loss: 1.2513 - accuracy: 0.6666 - val_loss: 1.1998 - val_accuracy: 0.6743\n",
            "\n",
            "Epoch 00026: loss improved from 1.26954 to 1.25125, saving model to checkpoint/cp.ckpt\n",
            "Epoch 27/30\n",
            "274/274 [==============================] - 5s 19ms/step - loss: 1.2328 - accuracy: 0.6662 - val_loss: 1.1841 - val_accuracy: 0.6834\n",
            "\n",
            "Epoch 00027: loss improved from 1.25125 to 1.23282, saving model to checkpoint/cp.ckpt\n",
            "Epoch 28/30\n",
            "274/274 [==============================] - 5s 19ms/step - loss: 1.2163 - accuracy: 0.6713 - val_loss: 1.1891 - val_accuracy: 0.6791\n",
            "\n",
            "Epoch 00028: loss improved from 1.23282 to 1.21627, saving model to checkpoint/cp.ckpt\n",
            "Epoch 29/30\n",
            "274/274 [==============================] - 5s 19ms/step - loss: 1.2022 - accuracy: 0.6755 - val_loss: 1.1991 - val_accuracy: 0.6752\n",
            "\n",
            "Epoch 00029: loss improved from 1.21627 to 1.20216, saving model to checkpoint/cp.ckpt\n",
            "Epoch 30/30\n",
            "274/274 [==============================] - 5s 19ms/step - loss: 1.2017 - accuracy: 0.6729 - val_loss: 1.1787 - val_accuracy: 0.6827\n",
            "\n",
            "Epoch 00030: loss improved from 1.20216 to 1.20171, saving model to checkpoint/cp.ckpt\n",
            "Epoch 1/30\n",
            "274/274 [==============================] - 9s 23ms/step - loss: 2.7717 - accuracy: 0.4800 - val_loss: 2.4307 - val_accuracy: 0.4862\n",
            "\n",
            "Epoch 00001: loss improved from inf to 2.77166, saving model to checkpoint/cp.ckpt\n",
            "Epoch 2/30\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 2.3181 - accuracy: 0.4992 - val_loss: 2.1104 - val_accuracy: 0.5097\n",
            "\n",
            "Epoch 00002: loss improved from 2.77166 to 2.31807, saving model to checkpoint/cp.ckpt\n",
            "Epoch 3/30\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 2.1337 - accuracy: 0.5138 - val_loss: 2.0004 - val_accuracy: 0.5221\n",
            "\n",
            "Epoch 00003: loss improved from 2.31807 to 2.13370, saving model to checkpoint/cp.ckpt\n",
            "Epoch 4/30\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 2.0342 - accuracy: 0.5218 - val_loss: 1.8608 - val_accuracy: 0.5440\n",
            "\n",
            "Epoch 00004: loss improved from 2.13370 to 2.03423, saving model to checkpoint/cp.ckpt\n",
            "Epoch 5/30\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 1.9480 - accuracy: 0.5298 - val_loss: 1.7649 - val_accuracy: 0.5557\n",
            "\n",
            "Epoch 00005: loss improved from 2.03423 to 1.94802, saving model to checkpoint/cp.ckpt\n",
            "Epoch 6/30\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 1.8645 - accuracy: 0.5431 - val_loss: 1.7091 - val_accuracy: 0.5632\n",
            "\n",
            "Epoch 00006: loss improved from 1.94802 to 1.86455, saving model to checkpoint/cp.ckpt\n",
            "Epoch 7/30\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 1.7918 - accuracy: 0.5569 - val_loss: 1.5917 - val_accuracy: 0.5895\n",
            "\n",
            "Epoch 00007: loss improved from 1.86455 to 1.79178, saving model to checkpoint/cp.ckpt\n",
            "Epoch 8/30\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 1.7075 - accuracy: 0.5723 - val_loss: 1.5147 - val_accuracy: 0.6016\n",
            "\n",
            "Epoch 00008: loss improved from 1.79178 to 1.70755, saving model to checkpoint/cp.ckpt\n",
            "Epoch 9/30\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 1.6325 - accuracy: 0.5854 - val_loss: 1.4786 - val_accuracy: 0.6146\n",
            "\n",
            "Epoch 00009: loss improved from 1.70755 to 1.63247, saving model to checkpoint/cp.ckpt\n",
            "Epoch 10/30\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 1.5806 - accuracy: 0.6017 - val_loss: 1.4211 - val_accuracy: 0.6304\n",
            "\n",
            "Epoch 00010: loss improved from 1.63247 to 1.58056, saving model to checkpoint/cp.ckpt\n",
            "Epoch 11/30\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 1.5426 - accuracy: 0.6081 - val_loss: 1.3771 - val_accuracy: 0.6421\n",
            "\n",
            "Epoch 00011: loss improved from 1.58056 to 1.54257, saving model to checkpoint/cp.ckpt\n",
            "Epoch 12/30\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 1.4928 - accuracy: 0.6159 - val_loss: 1.3619 - val_accuracy: 0.6405\n",
            "\n",
            "Epoch 00012: loss improved from 1.54257 to 1.49276, saving model to checkpoint/cp.ckpt\n",
            "Epoch 13/30\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 1.4443 - accuracy: 0.6303 - val_loss: 1.2899 - val_accuracy: 0.6537\n",
            "\n",
            "Epoch 00013: loss improved from 1.49276 to 1.44428, saving model to checkpoint/cp.ckpt\n",
            "Epoch 14/30\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 1.4149 - accuracy: 0.6342 - val_loss: 1.2839 - val_accuracy: 0.6597\n",
            "\n",
            "Epoch 00014: loss improved from 1.44428 to 1.41493, saving model to checkpoint/cp.ckpt\n",
            "Epoch 15/30\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 1.3858 - accuracy: 0.6438 - val_loss: 1.2968 - val_accuracy: 0.6487\n",
            "\n",
            "Epoch 00015: loss improved from 1.41493 to 1.38578, saving model to checkpoint/cp.ckpt\n",
            "Epoch 16/30\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 1.3466 - accuracy: 0.6511 - val_loss: 1.2432 - val_accuracy: 0.6649\n",
            "\n",
            "Epoch 00016: loss improved from 1.38578 to 1.34656, saving model to checkpoint/cp.ckpt\n",
            "Epoch 17/30\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 1.3200 - accuracy: 0.6585 - val_loss: 1.2318 - val_accuracy: 0.6729\n",
            "\n",
            "Epoch 00017: loss improved from 1.34656 to 1.32002, saving model to checkpoint/cp.ckpt\n",
            "Epoch 18/30\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 1.2940 - accuracy: 0.6590 - val_loss: 1.2270 - val_accuracy: 0.6709\n",
            "\n",
            "Epoch 00018: loss improved from 1.32002 to 1.29396, saving model to checkpoint/cp.ckpt\n",
            "Epoch 19/30\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 1.2675 - accuracy: 0.6650 - val_loss: 1.1916 - val_accuracy: 0.6864\n",
            "\n",
            "Epoch 00019: loss improved from 1.29396 to 1.26750, saving model to checkpoint/cp.ckpt\n",
            "Epoch 20/30\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 1.2372 - accuracy: 0.6750 - val_loss: 1.2101 - val_accuracy: 0.6784\n",
            "\n",
            "Epoch 00020: loss improved from 1.26750 to 1.23718, saving model to checkpoint/cp.ckpt\n",
            "Epoch 21/30\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 1.2141 - accuracy: 0.6795 - val_loss: 1.1944 - val_accuracy: 0.6779\n",
            "\n",
            "Epoch 00021: loss improved from 1.23718 to 1.21410, saving model to checkpoint/cp.ckpt\n",
            "Epoch 22/30\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 1.1881 - accuracy: 0.6847 - val_loss: 1.1657 - val_accuracy: 0.6873\n",
            "\n",
            "Epoch 00022: loss improved from 1.21410 to 1.18815, saving model to checkpoint/cp.ckpt\n",
            "Epoch 23/30\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 1.1634 - accuracy: 0.6908 - val_loss: 1.1883 - val_accuracy: 0.6786\n",
            "\n",
            "Epoch 00023: loss improved from 1.18815 to 1.16345, saving model to checkpoint/cp.ckpt\n",
            "Epoch 24/30\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 1.1634 - accuracy: 0.6900 - val_loss: 1.1755 - val_accuracy: 0.6830\n",
            "\n",
            "Epoch 00024: loss improved from 1.16345 to 1.16338, saving model to checkpoint/cp.ckpt\n",
            "Epoch 25/30\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 1.1362 - accuracy: 0.6974 - val_loss: 1.1682 - val_accuracy: 0.6818\n",
            "\n",
            "Epoch 00025: loss improved from 1.16338 to 1.13619, saving model to checkpoint/cp.ckpt\n",
            "Epoch 26/30\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 1.1134 - accuracy: 0.7020 - val_loss: 1.1826 - val_accuracy: 0.6855\n",
            "\n",
            "Epoch 00026: loss improved from 1.13619 to 1.11344, saving model to checkpoint/cp.ckpt\n",
            "Epoch 27/30\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 1.0965 - accuracy: 0.7024 - val_loss: 1.1839 - val_accuracy: 0.6841\n",
            "\n",
            "Epoch 00027: loss improved from 1.11344 to 1.09654, saving model to checkpoint/cp.ckpt\n",
            "Epoch 28/30\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 1.0848 - accuracy: 0.7070 - val_loss: 1.1540 - val_accuracy: 0.6955\n",
            "\n",
            "Epoch 00028: loss improved from 1.09654 to 1.08479, saving model to checkpoint/cp.ckpt\n",
            "Epoch 29/30\n",
            "274/274 [==============================] - 5s 17ms/step - loss: 1.0731 - accuracy: 0.7111 - val_loss: 1.1538 - val_accuracy: 0.6944\n",
            "\n",
            "Epoch 00029: loss improved from 1.08479 to 1.07312, saving model to checkpoint/cp.ckpt\n",
            "Epoch 30/30\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 1.0518 - accuracy: 0.7155 - val_loss: 1.1549 - val_accuracy: 0.6866\n",
            "\n",
            "Epoch 00030: loss improved from 1.07312 to 1.05181, saving model to checkpoint/cp.ckpt\n",
            "Epoch 1/30\n",
            "274/274 [==============================] - 7s 16ms/step - loss: 2.6094 - accuracy: 0.4829 - val_loss: 2.1378 - val_accuracy: 0.5294\n",
            "\n",
            "Epoch 00001: loss improved from inf to 2.60939, saving model to checkpoint/cp.ckpt\n",
            "Epoch 2/30\n",
            "274/274 [==============================] - 4s 14ms/step - loss: 2.1261 - accuracy: 0.5189 - val_loss: 1.8716 - val_accuracy: 0.5479\n",
            "\n",
            "Epoch 00002: loss improved from 2.60939 to 2.12612, saving model to checkpoint/cp.ckpt\n",
            "Epoch 3/30\n",
            "274/274 [==============================] - 4s 14ms/step - loss: 1.9648 - accuracy: 0.5342 - val_loss: 1.7820 - val_accuracy: 0.5735\n",
            "\n",
            "Epoch 00003: loss improved from 2.12612 to 1.96482, saving model to checkpoint/cp.ckpt\n",
            "Epoch 4/30\n",
            "274/274 [==============================] - 4s 15ms/step - loss: 1.8663 - accuracy: 0.5489 - val_loss: 1.6972 - val_accuracy: 0.5742\n",
            "\n",
            "Epoch 00004: loss improved from 1.96482 to 1.86626, saving model to checkpoint/cp.ckpt\n",
            "Epoch 5/30\n",
            "274/274 [==============================] - 4s 14ms/step - loss: 1.7857 - accuracy: 0.5589 - val_loss: 1.6092 - val_accuracy: 0.5959\n",
            "\n",
            "Epoch 00005: loss improved from 1.86626 to 1.78573, saving model to checkpoint/cp.ckpt\n",
            "Epoch 6/30\n",
            "274/274 [==============================] - 4s 14ms/step - loss: 1.7261 - accuracy: 0.5713 - val_loss: 1.4992 - val_accuracy: 0.6114\n",
            "\n",
            "Epoch 00006: loss improved from 1.78573 to 1.72610, saving model to checkpoint/cp.ckpt\n",
            "Epoch 7/30\n",
            "274/274 [==============================] - 4s 14ms/step - loss: 1.6711 - accuracy: 0.5818 - val_loss: 1.4360 - val_accuracy: 0.6238\n",
            "\n",
            "Epoch 00007: loss improved from 1.72610 to 1.67108, saving model to checkpoint/cp.ckpt\n",
            "Epoch 8/30\n",
            "274/274 [==============================] - 4s 15ms/step - loss: 1.6275 - accuracy: 0.5918 - val_loss: 1.3857 - val_accuracy: 0.6295\n",
            "\n",
            "Epoch 00008: loss improved from 1.67108 to 1.62752, saving model to checkpoint/cp.ckpt\n",
            "Epoch 9/30\n",
            "274/274 [==============================] - 4s 14ms/step - loss: 1.5907 - accuracy: 0.6004 - val_loss: 1.3642 - val_accuracy: 0.6329\n",
            "\n",
            "Epoch 00009: loss improved from 1.62752 to 1.59072, saving model to checkpoint/cp.ckpt\n",
            "Epoch 10/30\n",
            "274/274 [==============================] - 4s 14ms/step - loss: 1.5567 - accuracy: 0.6047 - val_loss: 1.3199 - val_accuracy: 0.6405\n",
            "\n",
            "Epoch 00010: loss improved from 1.59072 to 1.55672, saving model to checkpoint/cp.ckpt\n",
            "Epoch 11/30\n",
            "274/274 [==============================] - 4s 14ms/step - loss: 1.5313 - accuracy: 0.6091 - val_loss: 1.2839 - val_accuracy: 0.6546\n",
            "\n",
            "Epoch 00011: loss improved from 1.55672 to 1.53135, saving model to checkpoint/cp.ckpt\n",
            "Epoch 12/30\n",
            "274/274 [==============================] - 4s 14ms/step - loss: 1.5019 - accuracy: 0.6171 - val_loss: 1.2617 - val_accuracy: 0.6539\n",
            "\n",
            "Epoch 00012: loss improved from 1.53135 to 1.50195, saving model to checkpoint/cp.ckpt\n",
            "Epoch 13/30\n",
            "274/274 [==============================] - 4s 14ms/step - loss: 1.4804 - accuracy: 0.6243 - val_loss: 1.2351 - val_accuracy: 0.6677\n",
            "\n",
            "Epoch 00013: loss improved from 1.50195 to 1.48043, saving model to checkpoint/cp.ckpt\n",
            "Epoch 14/30\n",
            "274/274 [==============================] - 4s 15ms/step - loss: 1.4575 - accuracy: 0.6337 - val_loss: 1.2006 - val_accuracy: 0.6690\n",
            "\n",
            "Epoch 00014: loss improved from 1.48043 to 1.45755, saving model to checkpoint/cp.ckpt\n",
            "Epoch 15/30\n",
            "274/274 [==============================] - 4s 14ms/step - loss: 1.4366 - accuracy: 0.6345 - val_loss: 1.1616 - val_accuracy: 0.6809\n",
            "\n",
            "Epoch 00015: loss improved from 1.45755 to 1.43664, saving model to checkpoint/cp.ckpt\n",
            "Epoch 16/30\n",
            "274/274 [==============================] - 4s 14ms/step - loss: 1.4197 - accuracy: 0.6382 - val_loss: 1.1428 - val_accuracy: 0.6875\n",
            "\n",
            "Epoch 00016: loss improved from 1.43664 to 1.41967, saving model to checkpoint/cp.ckpt\n",
            "Epoch 17/30\n",
            "274/274 [==============================] - 4s 15ms/step - loss: 1.3923 - accuracy: 0.6499 - val_loss: 1.1308 - val_accuracy: 0.6853\n",
            "\n",
            "Epoch 00017: loss improved from 1.41967 to 1.39229, saving model to checkpoint/cp.ckpt\n",
            "Epoch 18/30\n",
            "274/274 [==============================] - 4s 15ms/step - loss: 1.3824 - accuracy: 0.6464 - val_loss: 1.1313 - val_accuracy: 0.6882\n",
            "\n",
            "Epoch 00018: loss improved from 1.39229 to 1.38239, saving model to checkpoint/cp.ckpt\n",
            "Epoch 19/30\n",
            "274/274 [==============================] - 4s 15ms/step - loss: 1.3618 - accuracy: 0.6514 - val_loss: 1.0937 - val_accuracy: 0.7033\n",
            "\n",
            "Epoch 00019: loss improved from 1.38239 to 1.36181, saving model to checkpoint/cp.ckpt\n",
            "Epoch 20/30\n",
            "274/274 [==============================] - 4s 15ms/step - loss: 1.3461 - accuracy: 0.6589 - val_loss: 1.0666 - val_accuracy: 0.7081\n",
            "\n",
            "Epoch 00020: loss improved from 1.36181 to 1.34614, saving model to checkpoint/cp.ckpt\n",
            "Epoch 21/30\n",
            "274/274 [==============================] - 4s 15ms/step - loss: 1.3411 - accuracy: 0.6594 - val_loss: 1.0751 - val_accuracy: 0.6949\n",
            "\n",
            "Epoch 00021: loss improved from 1.34614 to 1.34113, saving model to checkpoint/cp.ckpt\n",
            "Epoch 22/30\n",
            "274/274 [==============================] - 4s 15ms/step - loss: 1.3155 - accuracy: 0.6657 - val_loss: 1.0460 - val_accuracy: 0.7070\n",
            "\n",
            "Epoch 00022: loss improved from 1.34113 to 1.31552, saving model to checkpoint/cp.ckpt\n",
            "Epoch 23/30\n",
            "274/274 [==============================] - 4s 15ms/step - loss: 1.3061 - accuracy: 0.6685 - val_loss: 1.0110 - val_accuracy: 0.7143\n",
            "\n",
            "Epoch 00023: loss improved from 1.31552 to 1.30610, saving model to checkpoint/cp.ckpt\n",
            "Epoch 24/30\n",
            "274/274 [==============================] - 4s 15ms/step - loss: 1.2969 - accuracy: 0.6715 - val_loss: 1.0124 - val_accuracy: 0.7136\n",
            "\n",
            "Epoch 00024: loss improved from 1.30610 to 1.29694, saving model to checkpoint/cp.ckpt\n",
            "Epoch 25/30\n",
            "274/274 [==============================] - 4s 15ms/step - loss: 1.2879 - accuracy: 0.6720 - val_loss: 1.0258 - val_accuracy: 0.7106\n",
            "\n",
            "Epoch 00025: loss improved from 1.29694 to 1.28792, saving model to checkpoint/cp.ckpt\n",
            "Epoch 26/30\n",
            "274/274 [==============================] - 4s 15ms/step - loss: 1.2764 - accuracy: 0.6750 - val_loss: 1.0107 - val_accuracy: 0.7175\n",
            "\n",
            "Epoch 00026: loss improved from 1.28792 to 1.27637, saving model to checkpoint/cp.ckpt\n",
            "Epoch 27/30\n",
            "274/274 [==============================] - 4s 15ms/step - loss: 1.2662 - accuracy: 0.6757 - val_loss: 1.0020 - val_accuracy: 0.7104\n",
            "\n",
            "Epoch 00027: loss improved from 1.27637 to 1.26625, saving model to checkpoint/cp.ckpt\n",
            "Epoch 28/30\n",
            "274/274 [==============================] - 4s 15ms/step - loss: 1.2609 - accuracy: 0.6763 - val_loss: 0.9623 - val_accuracy: 0.7362\n",
            "\n",
            "Epoch 00028: loss improved from 1.26625 to 1.26086, saving model to checkpoint/cp.ckpt\n",
            "Epoch 29/30\n",
            "274/274 [==============================] - 4s 15ms/step - loss: 1.2540 - accuracy: 0.6787 - val_loss: 0.9746 - val_accuracy: 0.7246\n",
            "\n",
            "Epoch 00029: loss improved from 1.26086 to 1.25405, saving model to checkpoint/cp.ckpt\n",
            "Epoch 30/30\n",
            "274/274 [==============================] - 4s 15ms/step - loss: 1.2449 - accuracy: 0.6849 - val_loss: 0.9708 - val_accuracy: 0.7232\n",
            "\n",
            "Epoch 00030: loss improved from 1.25405 to 1.24492, saving model to checkpoint/cp.ckpt\n",
            "Epoch 1/30\n",
            "274/274 [==============================] - 9s 22ms/step - loss: 2.4923 - accuracy: 0.4935 - val_loss: 2.0525 - val_accuracy: 0.5321\n",
            "\n",
            "Epoch 00001: loss improved from inf to 2.49231, saving model to checkpoint/cp.ckpt\n",
            "Epoch 2/30\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 2.0085 - accuracy: 0.5327 - val_loss: 1.8163 - val_accuracy: 0.5755\n",
            "\n",
            "Epoch 00002: loss improved from 2.49231 to 2.00850, saving model to checkpoint/cp.ckpt\n",
            "Epoch 3/30\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 1.8366 - accuracy: 0.5583 - val_loss: 1.6089 - val_accuracy: 0.5890\n",
            "\n",
            "Epoch 00003: loss improved from 2.00850 to 1.83664, saving model to checkpoint/cp.ckpt\n",
            "Epoch 4/30\n",
            "274/274 [==============================] - 5s 19ms/step - loss: 1.7197 - accuracy: 0.5725 - val_loss: 1.5288 - val_accuracy: 0.6169\n",
            "\n",
            "Epoch 00004: loss improved from 1.83664 to 1.71970, saving model to checkpoint/cp.ckpt\n",
            "Epoch 5/30\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 1.6329 - accuracy: 0.5872 - val_loss: 1.4054 - val_accuracy: 0.6352\n",
            "\n",
            "Epoch 00005: loss improved from 1.71970 to 1.63291, saving model to checkpoint/cp.ckpt\n",
            "Epoch 6/30\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 1.5680 - accuracy: 0.6034 - val_loss: 1.3501 - val_accuracy: 0.6489\n",
            "\n",
            "Epoch 00006: loss improved from 1.63291 to 1.56799, saving model to checkpoint/cp.ckpt\n",
            "Epoch 7/30\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 1.5252 - accuracy: 0.6152 - val_loss: 1.2401 - val_accuracy: 0.6693\n",
            "\n",
            "Epoch 00007: loss improved from 1.56799 to 1.52524, saving model to checkpoint/cp.ckpt\n",
            "Epoch 8/30\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 1.4704 - accuracy: 0.6300 - val_loss: 1.2271 - val_accuracy: 0.6686\n",
            "\n",
            "Epoch 00008: loss improved from 1.52524 to 1.47042, saving model to checkpoint/cp.ckpt\n",
            "Epoch 9/30\n",
            "274/274 [==============================] - 5s 19ms/step - loss: 1.4334 - accuracy: 0.6385 - val_loss: 1.1651 - val_accuracy: 0.6827\n",
            "\n",
            "Epoch 00009: loss improved from 1.47042 to 1.43340, saving model to checkpoint/cp.ckpt\n",
            "Epoch 10/30\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 1.3952 - accuracy: 0.6483 - val_loss: 1.1298 - val_accuracy: 0.6969\n",
            "\n",
            "Epoch 00010: loss improved from 1.43340 to 1.39520, saving model to checkpoint/cp.ckpt\n",
            "Epoch 11/30\n",
            "274/274 [==============================] - 5s 19ms/step - loss: 1.3690 - accuracy: 0.6564 - val_loss: 1.1022 - val_accuracy: 0.7003\n",
            "\n",
            "Epoch 00011: loss improved from 1.39520 to 1.36896, saving model to checkpoint/cp.ckpt\n",
            "Epoch 12/30\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 1.3424 - accuracy: 0.6670 - val_loss: 1.0521 - val_accuracy: 0.7067\n",
            "\n",
            "Epoch 00012: loss improved from 1.36896 to 1.34245, saving model to checkpoint/cp.ckpt\n",
            "Epoch 13/30\n",
            "274/274 [==============================] - 5s 19ms/step - loss: 1.3130 - accuracy: 0.6696 - val_loss: 1.0224 - val_accuracy: 0.7202\n",
            "\n",
            "Epoch 00013: loss improved from 1.34245 to 1.31296, saving model to checkpoint/cp.ckpt\n",
            "Epoch 14/30\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 1.2950 - accuracy: 0.6747 - val_loss: 1.0198 - val_accuracy: 0.7111\n",
            "\n",
            "Epoch 00014: loss improved from 1.31296 to 1.29504, saving model to checkpoint/cp.ckpt\n",
            "Epoch 15/30\n",
            "274/274 [==============================] - 5s 19ms/step - loss: 1.2724 - accuracy: 0.6810 - val_loss: 1.0177 - val_accuracy: 0.7166\n",
            "\n",
            "Epoch 00015: loss improved from 1.29504 to 1.27239, saving model to checkpoint/cp.ckpt\n",
            "Epoch 16/30\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 1.2474 - accuracy: 0.6862 - val_loss: 0.9718 - val_accuracy: 0.7310\n",
            "\n",
            "Epoch 00016: loss improved from 1.27239 to 1.24740, saving model to checkpoint/cp.ckpt\n",
            "Epoch 17/30\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 1.2402 - accuracy: 0.6890 - val_loss: 0.9578 - val_accuracy: 0.7253\n",
            "\n",
            "Epoch 00017: loss improved from 1.24740 to 1.24018, saving model to checkpoint/cp.ckpt\n",
            "Epoch 18/30\n",
            "274/274 [==============================] - 5s 19ms/step - loss: 1.2193 - accuracy: 0.6931 - val_loss: 0.9207 - val_accuracy: 0.7367\n",
            "\n",
            "Epoch 00018: loss improved from 1.24018 to 1.21929, saving model to checkpoint/cp.ckpt\n",
            "Epoch 19/30\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 1.2012 - accuracy: 0.6990 - val_loss: 0.9091 - val_accuracy: 0.7422\n",
            "\n",
            "Epoch 00019: loss improved from 1.21929 to 1.20125, saving model to checkpoint/cp.ckpt\n",
            "Epoch 20/30\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 1.1867 - accuracy: 0.6999 - val_loss: 0.8923 - val_accuracy: 0.7479\n",
            "\n",
            "Epoch 00020: loss improved from 1.20125 to 1.18674, saving model to checkpoint/cp.ckpt\n",
            "Epoch 21/30\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 1.1767 - accuracy: 0.7049 - val_loss: 0.8778 - val_accuracy: 0.7472\n",
            "\n",
            "Epoch 00021: loss improved from 1.18674 to 1.17669, saving model to checkpoint/cp.ckpt\n",
            "Epoch 22/30\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 1.1601 - accuracy: 0.7067 - val_loss: 0.8747 - val_accuracy: 0.7470\n",
            "\n",
            "Epoch 00022: loss improved from 1.17669 to 1.16008, saving model to checkpoint/cp.ckpt\n",
            "Epoch 23/30\n",
            "274/274 [==============================] - 5s 19ms/step - loss: 1.1526 - accuracy: 0.7098 - val_loss: 0.8427 - val_accuracy: 0.7525\n",
            "\n",
            "Epoch 00023: loss improved from 1.16008 to 1.15263, saving model to checkpoint/cp.ckpt\n",
            "Epoch 24/30\n",
            "274/274 [==============================] - 5s 19ms/step - loss: 1.1355 - accuracy: 0.7147 - val_loss: 0.8314 - val_accuracy: 0.7602\n",
            "\n",
            "Epoch 00024: loss improved from 1.15263 to 1.13551, saving model to checkpoint/cp.ckpt\n",
            "Epoch 25/30\n",
            "274/274 [==============================] - 5s 19ms/step - loss: 1.1211 - accuracy: 0.7181 - val_loss: 0.8259 - val_accuracy: 0.7653\n",
            "\n",
            "Epoch 00025: loss improved from 1.13551 to 1.12107, saving model to checkpoint/cp.ckpt\n",
            "Epoch 26/30\n",
            "274/274 [==============================] - 5s 20ms/step - loss: 1.1173 - accuracy: 0.7195 - val_loss: 0.8046 - val_accuracy: 0.7696\n",
            "\n",
            "Epoch 00026: loss improved from 1.12107 to 1.11734, saving model to checkpoint/cp.ckpt\n",
            "Epoch 27/30\n",
            "274/274 [==============================] - 5s 19ms/step - loss: 1.1076 - accuracy: 0.7251 - val_loss: 0.7926 - val_accuracy: 0.7655\n",
            "\n",
            "Epoch 00027: loss improved from 1.11734 to 1.10756, saving model to checkpoint/cp.ckpt\n",
            "Epoch 28/30\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 1.0954 - accuracy: 0.7251 - val_loss: 0.8129 - val_accuracy: 0.7637\n",
            "\n",
            "Epoch 00028: loss improved from 1.10756 to 1.09544, saving model to checkpoint/cp.ckpt\n",
            "Epoch 29/30\n",
            "274/274 [==============================] - 5s 18ms/step - loss: 1.0854 - accuracy: 0.7243 - val_loss: 0.7727 - val_accuracy: 0.7767\n",
            "\n",
            "Epoch 00029: loss improved from 1.09544 to 1.08543, saving model to checkpoint/cp.ckpt\n",
            "Epoch 30/30\n",
            "274/274 [==============================] - 5s 19ms/step - loss: 1.0774 - accuracy: 0.7298 - val_loss: 0.7687 - val_accuracy: 0.7771\n",
            "\n",
            "Epoch 00030: loss improved from 1.08543 to 1.07742, saving model to checkpoint/cp.ckpt\n",
            "Epoch 1/30\n",
            "274/274 [==============================] - 10s 26ms/step - loss: 2.4265 - accuracy: 0.4963 - val_loss: 1.9599 - val_accuracy: 0.5506\n",
            "\n",
            "Epoch 00001: loss improved from inf to 2.42648, saving model to checkpoint/cp.ckpt\n",
            "Epoch 2/30\n",
            "274/274 [==============================] - 6s 21ms/step - loss: 1.8831 - accuracy: 0.5518 - val_loss: 1.6801 - val_accuracy: 0.5776\n",
            "\n",
            "Epoch 00002: loss improved from 2.42648 to 1.88314, saving model to checkpoint/cp.ckpt\n",
            "Epoch 3/30\n",
            "274/274 [==============================] - 6s 20ms/step - loss: 1.6719 - accuracy: 0.5861 - val_loss: 1.4778 - val_accuracy: 0.6213\n",
            "\n",
            "Epoch 00003: loss improved from 1.88314 to 1.67193, saving model to checkpoint/cp.ckpt\n",
            "Epoch 4/30\n",
            "274/274 [==============================] - 6s 20ms/step - loss: 1.5412 - accuracy: 0.6170 - val_loss: 1.3341 - val_accuracy: 0.6352\n",
            "\n",
            "Epoch 00004: loss improved from 1.67193 to 1.54123, saving model to checkpoint/cp.ckpt\n",
            "Epoch 5/30\n",
            "274/274 [==============================] - 6s 21ms/step - loss: 1.4570 - accuracy: 0.6342 - val_loss: 1.2592 - val_accuracy: 0.6624\n",
            "\n",
            "Epoch 00005: loss improved from 1.54123 to 1.45703, saving model to checkpoint/cp.ckpt\n",
            "Epoch 6/30\n",
            "274/274 [==============================] - 6s 20ms/step - loss: 1.3832 - accuracy: 0.6543 - val_loss: 1.1736 - val_accuracy: 0.6807\n",
            "\n",
            "Epoch 00006: loss improved from 1.45703 to 1.38322, saving model to checkpoint/cp.ckpt\n",
            "Epoch 7/30\n",
            "274/274 [==============================] - 6s 20ms/step - loss: 1.3326 - accuracy: 0.6647 - val_loss: 1.1345 - val_accuracy: 0.6843\n",
            "\n",
            "Epoch 00007: loss improved from 1.38322 to 1.33257, saving model to checkpoint/cp.ckpt\n",
            "Epoch 8/30\n",
            "274/274 [==============================] - 6s 20ms/step - loss: 1.2920 - accuracy: 0.6746 - val_loss: 1.0795 - val_accuracy: 0.7017\n",
            "\n",
            "Epoch 00008: loss improved from 1.33257 to 1.29202, saving model to checkpoint/cp.ckpt\n",
            "Epoch 9/30\n",
            "274/274 [==============================] - 5s 20ms/step - loss: 1.2417 - accuracy: 0.6869 - val_loss: 1.0471 - val_accuracy: 0.7168\n",
            "\n",
            "Epoch 00009: loss improved from 1.29202 to 1.24167, saving model to checkpoint/cp.ckpt\n",
            "Epoch 10/30\n",
            "274/274 [==============================] - 6s 21ms/step - loss: 1.2219 - accuracy: 0.6943 - val_loss: 1.0168 - val_accuracy: 0.7182\n",
            "\n",
            "Epoch 00010: loss improved from 1.24167 to 1.22185, saving model to checkpoint/cp.ckpt\n",
            "Epoch 11/30\n",
            "274/274 [==============================] - 6s 20ms/step - loss: 1.1829 - accuracy: 0.7019 - val_loss: 0.9592 - val_accuracy: 0.7289\n",
            "\n",
            "Epoch 00011: loss improved from 1.22185 to 1.18290, saving model to checkpoint/cp.ckpt\n",
            "Epoch 12/30\n",
            "274/274 [==============================] - 6s 20ms/step - loss: 1.1553 - accuracy: 0.7105 - val_loss: 0.9548 - val_accuracy: 0.7314\n",
            "\n",
            "Epoch 00012: loss improved from 1.18290 to 1.15533, saving model to checkpoint/cp.ckpt\n",
            "Epoch 13/30\n",
            "274/274 [==============================] - 6s 20ms/step - loss: 1.1390 - accuracy: 0.7116 - val_loss: 0.9117 - val_accuracy: 0.7399\n",
            "\n",
            "Epoch 00013: loss improved from 1.15533 to 1.13898, saving model to checkpoint/cp.ckpt\n",
            "Epoch 14/30\n",
            "274/274 [==============================] - 6s 20ms/step - loss: 1.1025 - accuracy: 0.7203 - val_loss: 0.9071 - val_accuracy: 0.7424\n",
            "\n",
            "Epoch 00014: loss improved from 1.13898 to 1.10251, saving model to checkpoint/cp.ckpt\n",
            "Epoch 15/30\n",
            "274/274 [==============================] - 6s 21ms/step - loss: 1.0921 - accuracy: 0.7269 - val_loss: 0.9129 - val_accuracy: 0.7461\n",
            "\n",
            "Epoch 00015: loss improved from 1.10251 to 1.09207, saving model to checkpoint/cp.ckpt\n",
            "Epoch 16/30\n",
            "274/274 [==============================] - 6s 21ms/step - loss: 1.0734 - accuracy: 0.7330 - val_loss: 0.8622 - val_accuracy: 0.7550\n",
            "\n",
            "Epoch 00016: loss improved from 1.09207 to 1.07339, saving model to checkpoint/cp.ckpt\n",
            "Epoch 17/30\n",
            "274/274 [==============================] - 6s 20ms/step - loss: 1.0483 - accuracy: 0.7349 - val_loss: 0.8378 - val_accuracy: 0.7575\n",
            "\n",
            "Epoch 00017: loss improved from 1.07339 to 1.04833, saving model to checkpoint/cp.ckpt\n",
            "Epoch 18/30\n",
            "274/274 [==============================] - 6s 20ms/step - loss: 1.0240 - accuracy: 0.7382 - val_loss: 0.8462 - val_accuracy: 0.7570\n",
            "\n",
            "Epoch 00018: loss improved from 1.04833 to 1.02401, saving model to checkpoint/cp.ckpt\n",
            "Epoch 19/30\n",
            "274/274 [==============================] - 6s 21ms/step - loss: 1.0174 - accuracy: 0.7391 - val_loss: 0.7970 - val_accuracy: 0.7662\n",
            "\n",
            "Epoch 00019: loss improved from 1.02401 to 1.01744, saving model to checkpoint/cp.ckpt\n",
            "Epoch 20/30\n",
            "274/274 [==============================] - 6s 21ms/step - loss: 0.9946 - accuracy: 0.7499 - val_loss: 0.8178 - val_accuracy: 0.7579\n",
            "\n",
            "Epoch 00020: loss improved from 1.01744 to 0.99457, saving model to checkpoint/cp.ckpt\n",
            "Epoch 21/30\n",
            "274/274 [==============================] - 6s 21ms/step - loss: 0.9829 - accuracy: 0.7500 - val_loss: 0.7814 - val_accuracy: 0.7707\n",
            "\n",
            "Epoch 00021: loss improved from 0.99457 to 0.98294, saving model to checkpoint/cp.ckpt\n",
            "Epoch 22/30\n",
            "274/274 [==============================] - 6s 20ms/step - loss: 0.9764 - accuracy: 0.7501 - val_loss: 0.7766 - val_accuracy: 0.7717\n",
            "\n",
            "Epoch 00022: loss improved from 0.98294 to 0.97639, saving model to checkpoint/cp.ckpt\n",
            "Epoch 23/30\n",
            "274/274 [==============================] - 6s 21ms/step - loss: 0.9593 - accuracy: 0.7541 - val_loss: 0.7440 - val_accuracy: 0.7790\n",
            "\n",
            "Epoch 00023: loss improved from 0.97639 to 0.95925, saving model to checkpoint/cp.ckpt\n",
            "Epoch 24/30\n",
            "274/274 [==============================] - 6s 21ms/step - loss: 0.9396 - accuracy: 0.7610 - val_loss: 0.7517 - val_accuracy: 0.7785\n",
            "\n",
            "Epoch 00024: loss improved from 0.95925 to 0.93964, saving model to checkpoint/cp.ckpt\n",
            "Epoch 25/30\n",
            "274/274 [==============================] - 6s 20ms/step - loss: 0.9363 - accuracy: 0.7618 - val_loss: 0.7443 - val_accuracy: 0.7776\n",
            "\n",
            "Epoch 00025: loss improved from 0.93964 to 0.93635, saving model to checkpoint/cp.ckpt\n",
            "Epoch 26/30\n",
            "274/274 [==============================] - 6s 20ms/step - loss: 0.9165 - accuracy: 0.7677 - val_loss: 0.7211 - val_accuracy: 0.7902\n",
            "\n",
            "Epoch 00026: loss improved from 0.93635 to 0.91650, saving model to checkpoint/cp.ckpt\n",
            "Epoch 27/30\n",
            "274/274 [==============================] - 6s 20ms/step - loss: 0.9099 - accuracy: 0.7697 - val_loss: 0.6916 - val_accuracy: 0.7938\n",
            "\n",
            "Epoch 00027: loss improved from 0.91650 to 0.90991, saving model to checkpoint/cp.ckpt\n",
            "Epoch 28/30\n",
            "274/274 [==============================] - 6s 20ms/step - loss: 0.8962 - accuracy: 0.7726 - val_loss: 0.7362 - val_accuracy: 0.7865\n",
            "\n",
            "Epoch 00028: loss improved from 0.90991 to 0.89620, saving model to checkpoint/cp.ckpt\n",
            "Epoch 29/30\n",
            "274/274 [==============================] - 6s 21ms/step - loss: 0.8853 - accuracy: 0.7750 - val_loss: 0.6591 - val_accuracy: 0.8073\n",
            "\n",
            "Epoch 00029: loss improved from 0.89620 to 0.88531, saving model to checkpoint/cp.ckpt\n",
            "Epoch 30/30\n",
            "274/274 [==============================] - 6s 20ms/step - loss: 0.8832 - accuracy: 0.7756 - val_loss: 0.6912 - val_accuracy: 0.8016\n",
            "\n",
            "Epoch 00030: loss improved from 0.88531 to 0.88319, saving model to checkpoint/cp.ckpt\n",
            "Epoch 1/30\n",
            "274/274 [==============================] - 10s 26ms/step - loss: 2.4458 - accuracy: 0.4995 - val_loss: 1.9091 - val_accuracy: 0.5463\n",
            "\n",
            "Epoch 00001: loss improved from inf to 2.44584, saving model to checkpoint/cp.ckpt\n",
            "Epoch 2/30\n",
            "274/274 [==============================] - 6s 21ms/step - loss: 1.9580 - accuracy: 0.5379 - val_loss: 1.7290 - val_accuracy: 0.5735\n",
            "\n",
            "Epoch 00002: loss improved from 2.44584 to 1.95796, saving model to checkpoint/cp.ckpt\n",
            "Epoch 3/30\n",
            "274/274 [==============================] - 6s 21ms/step - loss: 1.8056 - accuracy: 0.5557 - val_loss: 1.5489 - val_accuracy: 0.6014\n",
            "\n",
            "Epoch 00003: loss improved from 1.95796 to 1.80558, saving model to checkpoint/cp.ckpt\n",
            "Epoch 4/30\n",
            "274/274 [==============================] - 6s 21ms/step - loss: 1.6946 - accuracy: 0.5773 - val_loss: 1.4498 - val_accuracy: 0.6181\n",
            "\n",
            "Epoch 00004: loss improved from 1.80558 to 1.69462, saving model to checkpoint/cp.ckpt\n",
            "Epoch 5/30\n",
            "274/274 [==============================] - 6s 21ms/step - loss: 1.6286 - accuracy: 0.5913 - val_loss: 1.3535 - val_accuracy: 0.6395\n",
            "\n",
            "Epoch 00005: loss improved from 1.69462 to 1.62859, saving model to checkpoint/cp.ckpt\n",
            "Epoch 6/30\n",
            "274/274 [==============================] - 6s 21ms/step - loss: 1.5553 - accuracy: 0.6079 - val_loss: 1.2812 - val_accuracy: 0.6546\n",
            "\n",
            "Epoch 00006: loss improved from 1.62859 to 1.55531, saving model to checkpoint/cp.ckpt\n",
            "Epoch 7/30\n",
            "274/274 [==============================] - 6s 21ms/step - loss: 1.5038 - accuracy: 0.6195 - val_loss: 1.2261 - val_accuracy: 0.6697\n",
            "\n",
            "Epoch 00007: loss improved from 1.55531 to 1.50375, saving model to checkpoint/cp.ckpt\n",
            "Epoch 8/30\n",
            "274/274 [==============================] - 6s 21ms/step - loss: 1.4592 - accuracy: 0.6302 - val_loss: 1.1383 - val_accuracy: 0.6823\n",
            "\n",
            "Epoch 00008: loss improved from 1.50375 to 1.45921, saving model to checkpoint/cp.ckpt\n",
            "Epoch 9/30\n",
            "274/274 [==============================] - 6s 21ms/step - loss: 1.4174 - accuracy: 0.6387 - val_loss: 1.0902 - val_accuracy: 0.6949\n",
            "\n",
            "Epoch 00009: loss improved from 1.45921 to 1.41739, saving model to checkpoint/cp.ckpt\n",
            "Epoch 10/30\n",
            "274/274 [==============================] - 6s 21ms/step - loss: 1.3875 - accuracy: 0.6505 - val_loss: 1.0607 - val_accuracy: 0.7054\n",
            "\n",
            "Epoch 00010: loss improved from 1.41739 to 1.38749, saving model to checkpoint/cp.ckpt\n",
            "Epoch 11/30\n",
            "274/274 [==============================] - 6s 21ms/step - loss: 1.3398 - accuracy: 0.6623 - val_loss: 1.0562 - val_accuracy: 0.7170\n",
            "\n",
            "Epoch 00011: loss improved from 1.38749 to 1.33983, saving model to checkpoint/cp.ckpt\n",
            "Epoch 12/30\n",
            "274/274 [==============================] - 6s 21ms/step - loss: 1.3184 - accuracy: 0.6717 - val_loss: 0.9921 - val_accuracy: 0.7237\n",
            "\n",
            "Epoch 00012: loss improved from 1.33983 to 1.31844, saving model to checkpoint/cp.ckpt\n",
            "Epoch 13/30\n",
            "274/274 [==============================] - 6s 21ms/step - loss: 1.2965 - accuracy: 0.6760 - val_loss: 0.9661 - val_accuracy: 0.7232\n",
            "\n",
            "Epoch 00013: loss improved from 1.31844 to 1.29651, saving model to checkpoint/cp.ckpt\n",
            "Epoch 14/30\n",
            "274/274 [==============================] - 6s 21ms/step - loss: 1.2720 - accuracy: 0.6791 - val_loss: 0.9649 - val_accuracy: 0.7323\n",
            "\n",
            "Epoch 00014: loss improved from 1.29651 to 1.27200, saving model to checkpoint/cp.ckpt\n",
            "Epoch 15/30\n",
            "274/274 [==============================] - 6s 21ms/step - loss: 1.2566 - accuracy: 0.6867 - val_loss: 0.9544 - val_accuracy: 0.7280\n",
            "\n",
            "Epoch 00015: loss improved from 1.27200 to 1.25657, saving model to checkpoint/cp.ckpt\n",
            "Epoch 16/30\n",
            "274/274 [==============================] - 6s 21ms/step - loss: 1.2275 - accuracy: 0.6938 - val_loss: 0.9206 - val_accuracy: 0.7390\n",
            "\n",
            "Epoch 00016: loss improved from 1.25657 to 1.22747, saving model to checkpoint/cp.ckpt\n",
            "Epoch 17/30\n",
            "274/274 [==============================] - 6s 21ms/step - loss: 1.2095 - accuracy: 0.6961 - val_loss: 0.9253 - val_accuracy: 0.7445\n",
            "\n",
            "Epoch 00017: loss improved from 1.22747 to 1.20948, saving model to checkpoint/cp.ckpt\n",
            "Epoch 18/30\n",
            "274/274 [==============================] - 6s 21ms/step - loss: 1.2036 - accuracy: 0.6971 - val_loss: 0.8642 - val_accuracy: 0.7520\n",
            "\n",
            "Epoch 00018: loss improved from 1.20948 to 1.20363, saving model to checkpoint/cp.ckpt\n",
            "Epoch 19/30\n",
            "274/274 [==============================] - 6s 21ms/step - loss: 1.1814 - accuracy: 0.7037 - val_loss: 0.8658 - val_accuracy: 0.7531\n",
            "\n",
            "Epoch 00019: loss improved from 1.20363 to 1.18141, saving model to checkpoint/cp.ckpt\n",
            "Epoch 20/30\n",
            "274/274 [==============================] - 6s 21ms/step - loss: 1.1688 - accuracy: 0.7064 - val_loss: 0.8594 - val_accuracy: 0.7534\n",
            "\n",
            "Epoch 00020: loss improved from 1.18141 to 1.16885, saving model to checkpoint/cp.ckpt\n",
            "Epoch 21/30\n",
            "274/274 [==============================] - 6s 21ms/step - loss: 1.1510 - accuracy: 0.7078 - val_loss: 0.8544 - val_accuracy: 0.7543\n",
            "\n",
            "Epoch 00021: loss improved from 1.16885 to 1.15100, saving model to checkpoint/cp.ckpt\n",
            "Epoch 22/30\n",
            "274/274 [==============================] - 6s 21ms/step - loss: 1.1459 - accuracy: 0.7108 - val_loss: 0.8411 - val_accuracy: 0.7559\n",
            "\n",
            "Epoch 00022: loss improved from 1.15100 to 1.14585, saving model to checkpoint/cp.ckpt\n",
            "Epoch 23/30\n",
            "274/274 [==============================] - 6s 21ms/step - loss: 1.1340 - accuracy: 0.7156 - val_loss: 0.8583 - val_accuracy: 0.7525\n",
            "\n",
            "Epoch 00023: loss improved from 1.14585 to 1.13397, saving model to checkpoint/cp.ckpt\n",
            "Epoch 24/30\n",
            "274/274 [==============================] - 6s 21ms/step - loss: 1.1178 - accuracy: 0.7170 - val_loss: 0.8153 - val_accuracy: 0.7657\n",
            "\n",
            "Epoch 00024: loss improved from 1.13397 to 1.11775, saving model to checkpoint/cp.ckpt\n",
            "Epoch 25/30\n",
            "274/274 [==============================] - 6s 21ms/step - loss: 1.0983 - accuracy: 0.7231 - val_loss: 0.8091 - val_accuracy: 0.7641\n",
            "\n",
            "Epoch 00025: loss improved from 1.11775 to 1.09833, saving model to checkpoint/cp.ckpt\n",
            "Epoch 26/30\n",
            "274/274 [==============================] - 6s 21ms/step - loss: 1.1086 - accuracy: 0.7204 - val_loss: 0.8014 - val_accuracy: 0.7723\n",
            "\n",
            "Epoch 00026: loss did not improve from 1.09833\n",
            "Epoch 27/30\n",
            "274/274 [==============================] - 6s 22ms/step - loss: 1.0804 - accuracy: 0.7276 - val_loss: 0.8140 - val_accuracy: 0.7669\n",
            "\n",
            "Epoch 00027: loss improved from 1.09833 to 1.08040, saving model to checkpoint/cp.ckpt\n",
            "Epoch 28/30\n",
            "274/274 [==============================] - 6s 21ms/step - loss: 1.0851 - accuracy: 0.7247 - val_loss: 0.7916 - val_accuracy: 0.7742\n",
            "\n",
            "Epoch 00028: loss did not improve from 1.08040\n",
            "Epoch 29/30\n",
            "274/274 [==============================] - 6s 21ms/step - loss: 1.0781 - accuracy: 0.7285 - val_loss: 0.7952 - val_accuracy: 0.7678\n",
            "\n",
            "Epoch 00029: loss improved from 1.08040 to 1.07806, saving model to checkpoint/cp.ckpt\n",
            "Epoch 30/30\n",
            "274/274 [==============================] - 6s 21ms/step - loss: 1.0649 - accuracy: 0.7337 - val_loss: 0.7624 - val_accuracy: 0.7765\n",
            "\n",
            "Epoch 00030: loss improved from 1.07806 to 1.06492, saving model to checkpoint/cp.ckpt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Sqxkt_QkEUa"
      },
      "source": [
        "# 저장한 모델 불러오기\n",
        "for i in range(8): # 모델 갯수\n",
        "    globals()[f'model{i}'] = load_model(f'models/model_{i}.h5')"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4TQvwEz73Kg"
      },
      "source": [
        "model0._name = 'Client0'\n",
        "model1._name = 'Client1'\n",
        "model2._name = 'Client2'\n",
        "model3._name = 'Client3'\n",
        "model4._name = 'Client4'\n",
        "model5._name = 'Client5'\n",
        "model6._name = 'Client6'\n",
        "model7._name = 'Client7'"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMyFk1x-kEYe"
      },
      "source": [
        "inputs = Input(shape=(n_steps, n_length, n_features))\n",
        "\n",
        "merge = concatenate([model0(inputs), model1(inputs), model2(inputs), model3(inputs), \n",
        "                    model4(inputs), model5(inputs), model6(inputs), model7(inputs)])\n",
        "hidden = Dense(10, activation='relu')(merge)\n",
        "output = Dense(61, activation='softmax')(hidden)\n",
        "model = tf.keras.models.Model(inputs=inputs, outputs=output)\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JyJBKsFxkEbn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43df0ffc-59c9-43d6-e8a6-c0de4d483a67"
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "stfold = StratifiedKFold(n_splits=5, shuffle=True)\n",
        "idx_iter = 0 \n",
        "skf_accuracy=[]\n",
        "\n",
        "for train_idx, valid_idx in stfold.split(X, y_train_total) : \n",
        "  Y_train, Y_valid = tf.gather(y, train_idx), tf.gather(y, valid_idx)\n",
        "  X_train, X_valid = tf.gather(X, train_idx), tf.gather(X, valid_idx)\n",
        "\n",
        "  checkpoint_path = \"checkpoint/cp.ckpt\"\n",
        "  cp_callback = ModelCheckpoint(filepath=checkpoint_path, monitor='val_loss', verbose=1, save_weights_only=True, save_best_only=True, mode='min')\n",
        "\n",
        "  early_stopping = EarlyStopping(monitor='loss', patience=5, mode='min')\n",
        "  model.fit(X_train, Y_train, epochs=10, batch_size=100, validation_split=0.02, callbacks=[early_stopping, cp_callback])\n",
        "  pred = model.predict(X_valid)\n",
        "\n",
        "  idx_iter += 1\n",
        "  print(\"\\n#### 교차 검증: {} #####\" .format(idx_iter))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "172/172 [==============================] - 48s 173ms/step - loss: 3.0717 - accuracy: 0.4756 - val_loss: 2.0638 - val_accuracy: 0.5257\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 2.06384, saving model to checkpoint/cp.ckpt\n",
            "Epoch 2/10\n",
            "172/172 [==============================] - 24s 139ms/step - loss: 1.9252 - accuracy: 0.5536 - val_loss: 1.6422 - val_accuracy: 0.6086\n",
            "\n",
            "Epoch 00002: val_loss improved from 2.06384 to 1.64220, saving model to checkpoint/cp.ckpt\n",
            "Epoch 3/10\n",
            "172/172 [==============================] - 24s 139ms/step - loss: 1.6387 - accuracy: 0.6027 - val_loss: 1.4085 - val_accuracy: 0.6486\n",
            "\n",
            "Epoch 00003: val_loss improved from 1.64220 to 1.40849, saving model to checkpoint/cp.ckpt\n",
            "Epoch 4/10\n",
            "172/172 [==============================] - 24s 140ms/step - loss: 1.4294 - accuracy: 0.6510 - val_loss: 1.2608 - val_accuracy: 0.6829\n",
            "\n",
            "Epoch 00004: val_loss improved from 1.40849 to 1.26082, saving model to checkpoint/cp.ckpt\n",
            "Epoch 5/10\n",
            "172/172 [==============================] - 24s 139ms/step - loss: 1.2877 - accuracy: 0.6838 - val_loss: 1.0998 - val_accuracy: 0.7000\n",
            "\n",
            "Epoch 00005: val_loss improved from 1.26082 to 1.09977, saving model to checkpoint/cp.ckpt\n",
            "Epoch 6/10\n",
            "172/172 [==============================] - 24s 139ms/step - loss: 1.2002 - accuracy: 0.6985 - val_loss: 1.0752 - val_accuracy: 0.7000\n",
            "\n",
            "Epoch 00006: val_loss improved from 1.09977 to 1.07516, saving model to checkpoint/cp.ckpt\n",
            "Epoch 7/10\n",
            "172/172 [==============================] - 24s 139ms/step - loss: 1.1454 - accuracy: 0.7107 - val_loss: 0.9920 - val_accuracy: 0.7086\n",
            "\n",
            "Epoch 00007: val_loss improved from 1.07516 to 0.99202, saving model to checkpoint/cp.ckpt\n",
            "Epoch 8/10\n",
            "172/172 [==============================] - 24s 139ms/step - loss: 1.0764 - accuracy: 0.7249 - val_loss: 0.9514 - val_accuracy: 0.7314\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.99202 to 0.95138, saving model to checkpoint/cp.ckpt\n",
            "Epoch 9/10\n",
            "172/172 [==============================] - 24s 139ms/step - loss: 1.0434 - accuracy: 0.7331 - val_loss: 0.9379 - val_accuracy: 0.7371\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.95138 to 0.93787, saving model to checkpoint/cp.ckpt\n",
            "Epoch 10/10\n",
            "172/172 [==============================] - 24s 139ms/step - loss: 0.9897 - accuracy: 0.7433 - val_loss: 0.9294 - val_accuracy: 0.7429\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.93787 to 0.92941, saving model to checkpoint/cp.ckpt\n",
            "\n",
            "#### 교차 검증: 1 #####\n",
            "Epoch 1/10\n",
            "172/172 [==============================] - 24s 140ms/step - loss: 1.0011 - accuracy: 0.7456 - val_loss: 0.9034 - val_accuracy: 0.7600\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.90337, saving model to checkpoint/cp.ckpt\n",
            "Epoch 2/10\n",
            "172/172 [==============================] - 24s 139ms/step - loss: 0.9637 - accuracy: 0.7518 - val_loss: 0.8967 - val_accuracy: 0.7600\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.90337 to 0.89666, saving model to checkpoint/cp.ckpt\n",
            "Epoch 3/10\n",
            "172/172 [==============================] - 24s 139ms/step - loss: 0.9255 - accuracy: 0.7641 - val_loss: 0.8993 - val_accuracy: 0.7371\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.89666\n",
            "Epoch 4/10\n",
            "172/172 [==============================] - 24s 139ms/step - loss: 0.9053 - accuracy: 0.7651 - val_loss: 0.8526 - val_accuracy: 0.7800\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.89666 to 0.85256, saving model to checkpoint/cp.ckpt\n",
            "Epoch 5/10\n",
            "172/172 [==============================] - 24s 139ms/step - loss: 0.8702 - accuracy: 0.7748 - val_loss: 0.7884 - val_accuracy: 0.7886\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.85256 to 0.78839, saving model to checkpoint/cp.ckpt\n",
            "Epoch 6/10\n",
            "172/172 [==============================] - 24s 140ms/step - loss: 0.8573 - accuracy: 0.7752 - val_loss: 0.7860 - val_accuracy: 0.7829\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.78839 to 0.78595, saving model to checkpoint/cp.ckpt\n",
            "Epoch 7/10\n",
            "172/172 [==============================] - 24s 139ms/step - loss: 0.8327 - accuracy: 0.7847 - val_loss: 0.8057 - val_accuracy: 0.7800\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.78595\n",
            "Epoch 8/10\n",
            "172/172 [==============================] - 24s 139ms/step - loss: 0.8080 - accuracy: 0.7906 - val_loss: 0.8053 - val_accuracy: 0.7743\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.78595\n",
            "Epoch 9/10\n",
            "172/172 [==============================] - 24s 139ms/step - loss: 0.7895 - accuracy: 0.7943 - val_loss: 0.7899 - val_accuracy: 0.7914\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.78595\n",
            "Epoch 10/10\n",
            "172/172 [==============================] - 24s 139ms/step - loss: 0.7640 - accuracy: 0.7994 - val_loss: 0.7615 - val_accuracy: 0.7800\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.78595 to 0.76155, saving model to checkpoint/cp.ckpt\n",
            "\n",
            "#### 교차 검증: 2 #####\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ow-WUCdr5jlQ"
      },
      "source": [
        "checkpoint_path = \"checkpoint/cp.ckpt\"\n",
        "cp_callback = ModelCheckpoint(filepath=checkpoint_path, monitor='val_loss', verbose=1, save_weights_only=True, save_best_only=True, mode='min')\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='loss', patience=30, mode='min')\n",
        "model.fit(X, y, epochs=epochs, batch_size=batch_size, validation_split=0.02, callbacks=[early_stopping, cp_callback])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUDiw-fA05uf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93c67665-d3d6-454b-b6b2-76ffe2e993e6"
      },
      "source": [
        "prediction = model.predict(test_X)\n",
        "prediction.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(782, 61)"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fP8wkIVHL79U",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "outputId": "9d31623d-73fb-4eb5-dd30-0514970b48e5"
      },
      "source": [
        "submission"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "      <th>45</th>\n",
              "      <th>46</th>\n",
              "      <th>47</th>\n",
              "      <th>48</th>\n",
              "      <th>49</th>\n",
              "      <th>50</th>\n",
              "      <th>51</th>\n",
              "      <th>52</th>\n",
              "      <th>53</th>\n",
              "      <th>54</th>\n",
              "      <th>55</th>\n",
              "      <th>56</th>\n",
              "      <th>57</th>\n",
              "      <th>58</th>\n",
              "      <th>59</th>\n",
              "      <th>60</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3125</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3126</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3127</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3128</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3129</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>777</th>\n",
              "      <td>3902</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>778</th>\n",
              "      <td>3903</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>779</th>\n",
              "      <td>3904</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>780</th>\n",
              "      <td>3905</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>781</th>\n",
              "      <td>3906</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>782 rows × 62 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       id  0  1  2  3  4  5  6  7  8  ...  51  52  53  54  55  56  57  58  59  60\n",
              "0    3125  0  0  0  0  0  0  0  0  0  ...   0   0   0   0   0   0   0   0   0   0\n",
              "1    3126  0  0  0  0  0  0  0  0  0  ...   0   0   0   0   0   0   0   0   0   0\n",
              "2    3127  0  0  0  0  0  0  0  0  0  ...   0   0   0   0   0   0   0   0   0   0\n",
              "3    3128  0  0  0  0  0  0  0  0  0  ...   0   0   0   0   0   0   0   0   0   0\n",
              "4    3129  0  0  0  0  0  0  0  0  0  ...   0   0   0   0   0   0   0   0   0   0\n",
              "..    ... .. .. .. .. .. .. .. .. ..  ...  ..  ..  ..  ..  ..  ..  ..  ..  ..  ..\n",
              "777  3902  0  0  0  0  0  0  0  0  0  ...   0   0   0   0   0   0   0   0   0   0\n",
              "778  3903  0  0  0  0  0  0  0  0  0  ...   0   0   0   0   0   0   0   0   0   0\n",
              "779  3904  0  0  0  0  0  0  0  0  0  ...   0   0   0   0   0   0   0   0   0   0\n",
              "780  3905  0  0  0  0  0  0  0  0  0  ...   0   0   0   0   0   0   0   0   0   0\n",
              "781  3906  0  0  0  0  0  0  0  0  0  ...   0   0   0   0   0   0   0   0   0   0\n",
              "\n",
              "[782 rows x 62 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngJyjpIBL972"
      },
      "source": [
        "submission.iloc[:,1:]=prediction"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABb6IgKAL_9N",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "outputId": "105a563f-f73b-4de1-e4f8-b296cec4c0c8"
      },
      "source": [
        "submission"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "      <th>45</th>\n",
              "      <th>46</th>\n",
              "      <th>47</th>\n",
              "      <th>48</th>\n",
              "      <th>49</th>\n",
              "      <th>50</th>\n",
              "      <th>51</th>\n",
              "      <th>52</th>\n",
              "      <th>53</th>\n",
              "      <th>54</th>\n",
              "      <th>55</th>\n",
              "      <th>56</th>\n",
              "      <th>57</th>\n",
              "      <th>58</th>\n",
              "      <th>59</th>\n",
              "      <th>60</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3125</td>\n",
              "      <td>0.000265</td>\n",
              "      <td>0.000555</td>\n",
              "      <td>1.831344e-04</td>\n",
              "      <td>7.889649e-05</td>\n",
              "      <td>0.000339</td>\n",
              "      <td>1.639430e-06</td>\n",
              "      <td>0.000017</td>\n",
              "      <td>0.000593</td>\n",
              "      <td>0.001640</td>\n",
              "      <td>0.003428</td>\n",
              "      <td>0.145505</td>\n",
              "      <td>0.448136</td>\n",
              "      <td>2.373632e-03</td>\n",
              "      <td>0.261655</td>\n",
              "      <td>0.034549</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>0.003687</td>\n",
              "      <td>0.000353</td>\n",
              "      <td>0.000075</td>\n",
              "      <td>0.000523</td>\n",
              "      <td>1.432443e-03</td>\n",
              "      <td>1.354106e-05</td>\n",
              "      <td>0.002138</td>\n",
              "      <td>0.028914</td>\n",
              "      <td>0.000006</td>\n",
              "      <td>0.012658</td>\n",
              "      <td>0.000922</td>\n",
              "      <td>0.000060</td>\n",
              "      <td>2.830876e-04</td>\n",
              "      <td>0.002979</td>\n",
              "      <td>0.002780</td>\n",
              "      <td>0.000084</td>\n",
              "      <td>0.000079</td>\n",
              "      <td>7.808581e-07</td>\n",
              "      <td>6.647074e-06</td>\n",
              "      <td>0.005904</td>\n",
              "      <td>0.000041</td>\n",
              "      <td>0.000336</td>\n",
              "      <td>0.000026</td>\n",
              "      <td>0.000009</td>\n",
              "      <td>2.173188e-07</td>\n",
              "      <td>0.000446</td>\n",
              "      <td>0.000768</td>\n",
              "      <td>0.000323</td>\n",
              "      <td>0.000558</td>\n",
              "      <td>0.000352</td>\n",
              "      <td>0.000022</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>0.000626</td>\n",
              "      <td>0.000003</td>\n",
              "      <td>1.696227e-02</td>\n",
              "      <td>3.425142e-03</td>\n",
              "      <td>1.007959e-03</td>\n",
              "      <td>0.005557</td>\n",
              "      <td>6.562532e-06</td>\n",
              "      <td>0.000063</td>\n",
              "      <td>0.000047</td>\n",
              "      <td>0.006832</td>\n",
              "      <td>0.000118</td>\n",
              "      <td>0.000227</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3126</td>\n",
              "      <td>0.000556</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>2.095053e-04</td>\n",
              "      <td>4.313315e-04</td>\n",
              "      <td>0.000123</td>\n",
              "      <td>5.296294e-04</td>\n",
              "      <td>0.000032</td>\n",
              "      <td>0.000074</td>\n",
              "      <td>0.000175</td>\n",
              "      <td>0.000135</td>\n",
              "      <td>0.000021</td>\n",
              "      <td>0.000048</td>\n",
              "      <td>1.984113e-05</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>0.000092</td>\n",
              "      <td>0.000029</td>\n",
              "      <td>0.000118</td>\n",
              "      <td>0.000064</td>\n",
              "      <td>0.000106</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000033</td>\n",
              "      <td>1.135462e-04</td>\n",
              "      <td>4.070189e-04</td>\n",
              "      <td>0.000084</td>\n",
              "      <td>0.000188</td>\n",
              "      <td>0.000151</td>\n",
              "      <td>0.990982</td>\n",
              "      <td>0.000446</td>\n",
              "      <td>0.000063</td>\n",
              "      <td>4.048764e-05</td>\n",
              "      <td>0.000078</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>0.000126</td>\n",
              "      <td>0.000031</td>\n",
              "      <td>8.743080e-05</td>\n",
              "      <td>1.150502e-04</td>\n",
              "      <td>0.000039</td>\n",
              "      <td>0.000080</td>\n",
              "      <td>0.000020</td>\n",
              "      <td>0.000011</td>\n",
              "      <td>0.000279</td>\n",
              "      <td>1.731373e-04</td>\n",
              "      <td>0.000011</td>\n",
              "      <td>0.000023</td>\n",
              "      <td>0.000050</td>\n",
              "      <td>0.000014</td>\n",
              "      <td>0.000036</td>\n",
              "      <td>0.000073</td>\n",
              "      <td>0.000189</td>\n",
              "      <td>0.001298</td>\n",
              "      <td>0.000632</td>\n",
              "      <td>7.771912e-05</td>\n",
              "      <td>1.483865e-05</td>\n",
              "      <td>3.218823e-05</td>\n",
              "      <td>0.000146</td>\n",
              "      <td>1.240795e-04</td>\n",
              "      <td>0.000153</td>\n",
              "      <td>0.000210</td>\n",
              "      <td>0.000078</td>\n",
              "      <td>0.000080</td>\n",
              "      <td>0.000414</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3127</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>0.018087</td>\n",
              "      <td>1.912311e-07</td>\n",
              "      <td>1.105333e-08</td>\n",
              "      <td>0.002896</td>\n",
              "      <td>7.745131e-07</td>\n",
              "      <td>0.000515</td>\n",
              "      <td>0.000070</td>\n",
              "      <td>0.000891</td>\n",
              "      <td>0.000011</td>\n",
              "      <td>0.001940</td>\n",
              "      <td>0.000019</td>\n",
              "      <td>1.822474e-06</td>\n",
              "      <td>0.000408</td>\n",
              "      <td>0.000035</td>\n",
              "      <td>0.000248</td>\n",
              "      <td>0.000027</td>\n",
              "      <td>0.000002</td>\n",
              "      <td>0.000335</td>\n",
              "      <td>0.000337</td>\n",
              "      <td>0.000322</td>\n",
              "      <td>4.157401e-07</td>\n",
              "      <td>3.641817e-09</td>\n",
              "      <td>0.000016</td>\n",
              "      <td>0.000476</td>\n",
              "      <td>0.005996</td>\n",
              "      <td>0.010276</td>\n",
              "      <td>0.001760</td>\n",
              "      <td>0.000473</td>\n",
              "      <td>8.238354e-07</td>\n",
              "      <td>0.000018</td>\n",
              "      <td>0.000032</td>\n",
              "      <td>0.000356</td>\n",
              "      <td>0.000058</td>\n",
              "      <td>1.101566e-04</td>\n",
              "      <td>8.234717e-07</td>\n",
              "      <td>0.000138</td>\n",
              "      <td>0.054571</td>\n",
              "      <td>0.000003</td>\n",
              "      <td>0.000002</td>\n",
              "      <td>0.000406</td>\n",
              "      <td>7.474995e-06</td>\n",
              "      <td>0.000087</td>\n",
              "      <td>0.002531</td>\n",
              "      <td>0.000812</td>\n",
              "      <td>0.885081</td>\n",
              "      <td>0.005538</td>\n",
              "      <td>0.000235</td>\n",
              "      <td>0.000182</td>\n",
              "      <td>0.000208</td>\n",
              "      <td>0.000116</td>\n",
              "      <td>5.496879e-07</td>\n",
              "      <td>1.892328e-06</td>\n",
              "      <td>2.177406e-08</td>\n",
              "      <td>0.000303</td>\n",
              "      <td>2.361107e-07</td>\n",
              "      <td>0.003556</td>\n",
              "      <td>0.000165</td>\n",
              "      <td>0.000039</td>\n",
              "      <td>0.000009</td>\n",
              "      <td>0.000189</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3128</td>\n",
              "      <td>0.001654</td>\n",
              "      <td>0.000060</td>\n",
              "      <td>5.610238e-04</td>\n",
              "      <td>8.784476e-04</td>\n",
              "      <td>0.000722</td>\n",
              "      <td>2.025258e-03</td>\n",
              "      <td>0.000137</td>\n",
              "      <td>0.000512</td>\n",
              "      <td>0.000841</td>\n",
              "      <td>0.000662</td>\n",
              "      <td>0.000159</td>\n",
              "      <td>0.000328</td>\n",
              "      <td>1.410022e-04</td>\n",
              "      <td>0.000062</td>\n",
              "      <td>0.000301</td>\n",
              "      <td>0.000239</td>\n",
              "      <td>0.000366</td>\n",
              "      <td>0.000438</td>\n",
              "      <td>0.000590</td>\n",
              "      <td>0.000138</td>\n",
              "      <td>0.000360</td>\n",
              "      <td>6.414252e-04</td>\n",
              "      <td>5.583558e-04</td>\n",
              "      <td>0.000647</td>\n",
              "      <td>0.000767</td>\n",
              "      <td>0.000917</td>\n",
              "      <td>0.964169</td>\n",
              "      <td>0.002015</td>\n",
              "      <td>0.000461</td>\n",
              "      <td>2.231048e-04</td>\n",
              "      <td>0.000280</td>\n",
              "      <td>0.000078</td>\n",
              "      <td>0.000724</td>\n",
              "      <td>0.000121</td>\n",
              "      <td>2.750729e-04</td>\n",
              "      <td>4.028830e-04</td>\n",
              "      <td>0.000199</td>\n",
              "      <td>0.000489</td>\n",
              "      <td>0.000088</td>\n",
              "      <td>0.000081</td>\n",
              "      <td>0.001506</td>\n",
              "      <td>7.646101e-04</td>\n",
              "      <td>0.000069</td>\n",
              "      <td>0.000144</td>\n",
              "      <td>0.000292</td>\n",
              "      <td>0.000183</td>\n",
              "      <td>0.000458</td>\n",
              "      <td>0.000236</td>\n",
              "      <td>0.000812</td>\n",
              "      <td>0.004295</td>\n",
              "      <td>0.001457</td>\n",
              "      <td>3.663954e-04</td>\n",
              "      <td>1.121139e-04</td>\n",
              "      <td>1.029428e-04</td>\n",
              "      <td>0.000715</td>\n",
              "      <td>3.136211e-04</td>\n",
              "      <td>0.001213</td>\n",
              "      <td>0.000505</td>\n",
              "      <td>0.000309</td>\n",
              "      <td>0.000369</td>\n",
              "      <td>0.001466</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3129</td>\n",
              "      <td>0.002359</td>\n",
              "      <td>0.000066</td>\n",
              "      <td>1.087026e-03</td>\n",
              "      <td>2.095253e-03</td>\n",
              "      <td>0.000872</td>\n",
              "      <td>2.219800e-03</td>\n",
              "      <td>0.000275</td>\n",
              "      <td>0.000520</td>\n",
              "      <td>0.001364</td>\n",
              "      <td>0.001112</td>\n",
              "      <td>0.000211</td>\n",
              "      <td>0.000453</td>\n",
              "      <td>2.413789e-04</td>\n",
              "      <td>0.000063</td>\n",
              "      <td>0.000759</td>\n",
              "      <td>0.000320</td>\n",
              "      <td>0.000697</td>\n",
              "      <td>0.000648</td>\n",
              "      <td>0.000765</td>\n",
              "      <td>0.000142</td>\n",
              "      <td>0.000245</td>\n",
              "      <td>7.824659e-04</td>\n",
              "      <td>2.835351e-03</td>\n",
              "      <td>0.000593</td>\n",
              "      <td>0.001250</td>\n",
              "      <td>0.000652</td>\n",
              "      <td>0.948661</td>\n",
              "      <td>0.001484</td>\n",
              "      <td>0.000630</td>\n",
              "      <td>4.374664e-04</td>\n",
              "      <td>0.000641</td>\n",
              "      <td>0.000185</td>\n",
              "      <td>0.000971</td>\n",
              "      <td>0.000258</td>\n",
              "      <td>5.684347e-04</td>\n",
              "      <td>8.516143e-04</td>\n",
              "      <td>0.000319</td>\n",
              "      <td>0.000438</td>\n",
              "      <td>0.000315</td>\n",
              "      <td>0.000196</td>\n",
              "      <td>0.001164</td>\n",
              "      <td>9.517164e-04</td>\n",
              "      <td>0.000156</td>\n",
              "      <td>0.000226</td>\n",
              "      <td>0.000362</td>\n",
              "      <td>0.000120</td>\n",
              "      <td>0.000285</td>\n",
              "      <td>0.000485</td>\n",
              "      <td>0.001144</td>\n",
              "      <td>0.005616</td>\n",
              "      <td>0.002748</td>\n",
              "      <td>6.485910e-04</td>\n",
              "      <td>2.037384e-04</td>\n",
              "      <td>2.713554e-04</td>\n",
              "      <td>0.000645</td>\n",
              "      <td>8.415297e-04</td>\n",
              "      <td>0.000833</td>\n",
              "      <td>0.001124</td>\n",
              "      <td>0.000602</td>\n",
              "      <td>0.000771</td>\n",
              "      <td>0.002220</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>777</th>\n",
              "      <td>3902</td>\n",
              "      <td>0.006149</td>\n",
              "      <td>0.001031</td>\n",
              "      <td>1.413109e-03</td>\n",
              "      <td>3.072813e-03</td>\n",
              "      <td>0.012486</td>\n",
              "      <td>3.655708e-03</td>\n",
              "      <td>0.004587</td>\n",
              "      <td>0.001102</td>\n",
              "      <td>0.005711</td>\n",
              "      <td>0.006047</td>\n",
              "      <td>0.000999</td>\n",
              "      <td>0.000958</td>\n",
              "      <td>1.520691e-03</td>\n",
              "      <td>0.000232</td>\n",
              "      <td>0.002731</td>\n",
              "      <td>0.007329</td>\n",
              "      <td>0.003626</td>\n",
              "      <td>0.001453</td>\n",
              "      <td>0.006676</td>\n",
              "      <td>0.003285</td>\n",
              "      <td>0.000977</td>\n",
              "      <td>1.604468e-03</td>\n",
              "      <td>2.022508e-02</td>\n",
              "      <td>0.002857</td>\n",
              "      <td>0.003622</td>\n",
              "      <td>0.003606</td>\n",
              "      <td>0.682333</td>\n",
              "      <td>0.002152</td>\n",
              "      <td>0.007510</td>\n",
              "      <td>2.535622e-03</td>\n",
              "      <td>0.004969</td>\n",
              "      <td>0.002304</td>\n",
              "      <td>0.011622</td>\n",
              "      <td>0.001539</td>\n",
              "      <td>1.555353e-02</td>\n",
              "      <td>2.922687e-03</td>\n",
              "      <td>0.002447</td>\n",
              "      <td>0.004265</td>\n",
              "      <td>0.004303</td>\n",
              "      <td>0.004633</td>\n",
              "      <td>0.004115</td>\n",
              "      <td>5.147588e-03</td>\n",
              "      <td>0.002438</td>\n",
              "      <td>0.003037</td>\n",
              "      <td>0.001620</td>\n",
              "      <td>0.001351</td>\n",
              "      <td>0.002173</td>\n",
              "      <td>0.005552</td>\n",
              "      <td>0.012806</td>\n",
              "      <td>0.017494</td>\n",
              "      <td>0.034678</td>\n",
              "      <td>1.392630e-03</td>\n",
              "      <td>1.512706e-03</td>\n",
              "      <td>8.699454e-04</td>\n",
              "      <td>0.000853</td>\n",
              "      <td>3.968467e-03</td>\n",
              "      <td>0.002419</td>\n",
              "      <td>0.017258</td>\n",
              "      <td>0.002146</td>\n",
              "      <td>0.003387</td>\n",
              "      <td>0.019740</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>778</th>\n",
              "      <td>3903</td>\n",
              "      <td>0.000788</td>\n",
              "      <td>0.000011</td>\n",
              "      <td>2.144309e-04</td>\n",
              "      <td>3.743960e-04</td>\n",
              "      <td>0.000312</td>\n",
              "      <td>5.790613e-04</td>\n",
              "      <td>0.000076</td>\n",
              "      <td>0.000108</td>\n",
              "      <td>0.000296</td>\n",
              "      <td>0.000272</td>\n",
              "      <td>0.000050</td>\n",
              "      <td>0.000092</td>\n",
              "      <td>4.593348e-05</td>\n",
              "      <td>0.000012</td>\n",
              "      <td>0.000157</td>\n",
              "      <td>0.000053</td>\n",
              "      <td>0.000157</td>\n",
              "      <td>0.000092</td>\n",
              "      <td>0.000173</td>\n",
              "      <td>0.000028</td>\n",
              "      <td>0.000054</td>\n",
              "      <td>1.391783e-04</td>\n",
              "      <td>5.735422e-04</td>\n",
              "      <td>0.000142</td>\n",
              "      <td>0.000335</td>\n",
              "      <td>0.000222</td>\n",
              "      <td>0.985053</td>\n",
              "      <td>0.000811</td>\n",
              "      <td>0.000119</td>\n",
              "      <td>5.213903e-05</td>\n",
              "      <td>0.000148</td>\n",
              "      <td>0.000034</td>\n",
              "      <td>0.000267</td>\n",
              "      <td>0.000041</td>\n",
              "      <td>1.572095e-04</td>\n",
              "      <td>1.101070e-04</td>\n",
              "      <td>0.000076</td>\n",
              "      <td>0.000186</td>\n",
              "      <td>0.000043</td>\n",
              "      <td>0.000023</td>\n",
              "      <td>0.000451</td>\n",
              "      <td>2.344291e-04</td>\n",
              "      <td>0.000019</td>\n",
              "      <td>0.000040</td>\n",
              "      <td>0.000076</td>\n",
              "      <td>0.000039</td>\n",
              "      <td>0.000078</td>\n",
              "      <td>0.000151</td>\n",
              "      <td>0.000420</td>\n",
              "      <td>0.002131</td>\n",
              "      <td>0.001275</td>\n",
              "      <td>1.257563e-04</td>\n",
              "      <td>3.431186e-05</td>\n",
              "      <td>4.683024e-05</td>\n",
              "      <td>0.000225</td>\n",
              "      <td>1.527090e-04</td>\n",
              "      <td>0.000280</td>\n",
              "      <td>0.000590</td>\n",
              "      <td>0.000143</td>\n",
              "      <td>0.000134</td>\n",
              "      <td>0.000880</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>779</th>\n",
              "      <td>3904</td>\n",
              "      <td>0.000866</td>\n",
              "      <td>0.000013</td>\n",
              "      <td>2.572958e-04</td>\n",
              "      <td>5.303622e-04</td>\n",
              "      <td>0.000259</td>\n",
              "      <td>8.426659e-04</td>\n",
              "      <td>0.000054</td>\n",
              "      <td>0.000176</td>\n",
              "      <td>0.000304</td>\n",
              "      <td>0.000205</td>\n",
              "      <td>0.000046</td>\n",
              "      <td>0.000101</td>\n",
              "      <td>4.826186e-05</td>\n",
              "      <td>0.000014</td>\n",
              "      <td>0.000146</td>\n",
              "      <td>0.000066</td>\n",
              "      <td>0.000193</td>\n",
              "      <td>0.000131</td>\n",
              "      <td>0.000156</td>\n",
              "      <td>0.000024</td>\n",
              "      <td>0.000074</td>\n",
              "      <td>2.123183e-04</td>\n",
              "      <td>5.207031e-04</td>\n",
              "      <td>0.000194</td>\n",
              "      <td>0.000292</td>\n",
              "      <td>0.000266</td>\n",
              "      <td>0.985284</td>\n",
              "      <td>0.000926</td>\n",
              "      <td>0.000105</td>\n",
              "      <td>8.349516e-05</td>\n",
              "      <td>0.000099</td>\n",
              "      <td>0.000022</td>\n",
              "      <td>0.000269</td>\n",
              "      <td>0.000042</td>\n",
              "      <td>1.319098e-04</td>\n",
              "      <td>1.654007e-04</td>\n",
              "      <td>0.000078</td>\n",
              "      <td>0.000149</td>\n",
              "      <td>0.000036</td>\n",
              "      <td>0.000020</td>\n",
              "      <td>0.000516</td>\n",
              "      <td>3.004065e-04</td>\n",
              "      <td>0.000021</td>\n",
              "      <td>0.000042</td>\n",
              "      <td>0.000122</td>\n",
              "      <td>0.000038</td>\n",
              "      <td>0.000101</td>\n",
              "      <td>0.000109</td>\n",
              "      <td>0.000324</td>\n",
              "      <td>0.002113</td>\n",
              "      <td>0.000673</td>\n",
              "      <td>1.447368e-04</td>\n",
              "      <td>3.381162e-05</td>\n",
              "      <td>5.182390e-05</td>\n",
              "      <td>0.000244</td>\n",
              "      <td>1.662027e-04</td>\n",
              "      <td>0.000300</td>\n",
              "      <td>0.000328</td>\n",
              "      <td>0.000120</td>\n",
              "      <td>0.000160</td>\n",
              "      <td>0.000688</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>780</th>\n",
              "      <td>3905</td>\n",
              "      <td>0.001238</td>\n",
              "      <td>0.045065</td>\n",
              "      <td>5.676660e-06</td>\n",
              "      <td>1.875046e-06</td>\n",
              "      <td>0.000198</td>\n",
              "      <td>9.776898e-06</td>\n",
              "      <td>0.017349</td>\n",
              "      <td>0.001672</td>\n",
              "      <td>0.000353</td>\n",
              "      <td>0.001787</td>\n",
              "      <td>0.000341</td>\n",
              "      <td>0.000014</td>\n",
              "      <td>3.456684e-07</td>\n",
              "      <td>0.000064</td>\n",
              "      <td>0.000013</td>\n",
              "      <td>0.001660</td>\n",
              "      <td>0.003977</td>\n",
              "      <td>0.000013</td>\n",
              "      <td>0.000950</td>\n",
              "      <td>0.004623</td>\n",
              "      <td>0.000054</td>\n",
              "      <td>8.858437e-06</td>\n",
              "      <td>1.255953e-07</td>\n",
              "      <td>0.000006</td>\n",
              "      <td>0.001159</td>\n",
              "      <td>0.001274</td>\n",
              "      <td>0.026262</td>\n",
              "      <td>0.001056</td>\n",
              "      <td>0.001097</td>\n",
              "      <td>4.673576e-07</td>\n",
              "      <td>0.000179</td>\n",
              "      <td>0.000306</td>\n",
              "      <td>0.000148</td>\n",
              "      <td>0.004292</td>\n",
              "      <td>1.238066e-03</td>\n",
              "      <td>9.075353e-06</td>\n",
              "      <td>0.000220</td>\n",
              "      <td>0.800772</td>\n",
              "      <td>0.000341</td>\n",
              "      <td>0.000031</td>\n",
              "      <td>0.030137</td>\n",
              "      <td>5.228310e-04</td>\n",
              "      <td>0.000043</td>\n",
              "      <td>0.000419</td>\n",
              "      <td>0.000135</td>\n",
              "      <td>0.015031</td>\n",
              "      <td>0.001980</td>\n",
              "      <td>0.009466</td>\n",
              "      <td>0.000197</td>\n",
              "      <td>0.000600</td>\n",
              "      <td>0.005305</td>\n",
              "      <td>2.414627e-06</td>\n",
              "      <td>3.700835e-07</td>\n",
              "      <td>8.505891e-08</td>\n",
              "      <td>0.001121</td>\n",
              "      <td>1.251332e-05</td>\n",
              "      <td>0.016227</td>\n",
              "      <td>0.000657</td>\n",
              "      <td>0.000307</td>\n",
              "      <td>0.000023</td>\n",
              "      <td>0.000024</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>781</th>\n",
              "      <td>3906</td>\n",
              "      <td>0.000425</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>1.082874e-04</td>\n",
              "      <td>2.191976e-04</td>\n",
              "      <td>0.000271</td>\n",
              "      <td>4.866865e-04</td>\n",
              "      <td>0.000033</td>\n",
              "      <td>0.000049</td>\n",
              "      <td>0.000145</td>\n",
              "      <td>0.000224</td>\n",
              "      <td>0.000027</td>\n",
              "      <td>0.000052</td>\n",
              "      <td>2.497953e-05</td>\n",
              "      <td>0.000006</td>\n",
              "      <td>0.000079</td>\n",
              "      <td>0.000029</td>\n",
              "      <td>0.000089</td>\n",
              "      <td>0.000070</td>\n",
              "      <td>0.000173</td>\n",
              "      <td>0.000025</td>\n",
              "      <td>0.000031</td>\n",
              "      <td>8.563462e-05</td>\n",
              "      <td>3.540244e-04</td>\n",
              "      <td>0.000109</td>\n",
              "      <td>0.000237</td>\n",
              "      <td>0.000166</td>\n",
              "      <td>0.990090</td>\n",
              "      <td>0.000408</td>\n",
              "      <td>0.000077</td>\n",
              "      <td>3.067979e-05</td>\n",
              "      <td>0.000094</td>\n",
              "      <td>0.000020</td>\n",
              "      <td>0.000147</td>\n",
              "      <td>0.000021</td>\n",
              "      <td>1.155107e-04</td>\n",
              "      <td>6.356618e-05</td>\n",
              "      <td>0.000031</td>\n",
              "      <td>0.000112</td>\n",
              "      <td>0.000020</td>\n",
              "      <td>0.000014</td>\n",
              "      <td>0.000287</td>\n",
              "      <td>2.033757e-04</td>\n",
              "      <td>0.000009</td>\n",
              "      <td>0.000020</td>\n",
              "      <td>0.000030</td>\n",
              "      <td>0.000020</td>\n",
              "      <td>0.000067</td>\n",
              "      <td>0.000065</td>\n",
              "      <td>0.000246</td>\n",
              "      <td>0.001287</td>\n",
              "      <td>0.001327</td>\n",
              "      <td>5.970255e-05</td>\n",
              "      <td>1.943196e-05</td>\n",
              "      <td>1.947754e-05</td>\n",
              "      <td>0.000106</td>\n",
              "      <td>6.370989e-05</td>\n",
              "      <td>0.000204</td>\n",
              "      <td>0.000437</td>\n",
              "      <td>0.000111</td>\n",
              "      <td>0.000084</td>\n",
              "      <td>0.000566</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>782 rows × 62 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       id         0         1  ...        58        59        60\n",
              "0    3125  0.000265  0.000555  ...  0.006832  0.000118  0.000227\n",
              "1    3126  0.000556  0.000004  ...  0.000078  0.000080  0.000414\n",
              "2    3127  0.000100  0.018087  ...  0.000039  0.000009  0.000189\n",
              "3    3128  0.001654  0.000060  ...  0.000309  0.000369  0.001466\n",
              "4    3129  0.002359  0.000066  ...  0.000602  0.000771  0.002220\n",
              "..    ...       ...       ...  ...       ...       ...       ...\n",
              "777  3902  0.006149  0.001031  ...  0.002146  0.003387  0.019740\n",
              "778  3903  0.000788  0.000011  ...  0.000143  0.000134  0.000880\n",
              "779  3904  0.000866  0.000013  ...  0.000120  0.000160  0.000688\n",
              "780  3905  0.001238  0.045065  ...  0.000307  0.000023  0.000024\n",
              "781  3906  0.000425  0.000005  ...  0.000111  0.000084  0.000566\n",
              "\n",
              "[782 rows x 62 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1ir4C2oMBFS"
      },
      "source": [
        "submission.to_csv('/content/drive/MyDrive/Colab Notebooks/AI hackathon/data/cnn_LSTM_stacked_9_skfold.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}